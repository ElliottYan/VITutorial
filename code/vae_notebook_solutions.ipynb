{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# How to implement a Variational Autoencoder (VAE)\n",
    "\n",
    "A variational autoencoder observes data, infers a latent code for it and tries to reconstruct the data from that latent code. In contrast to regular autoencoders, the code of the VAE is **random**. That means that when presented with the same input, the VAE will produce a slightly different code each time. This makes its decoding process more robust, since it has to deal with noisy code.\n",
    "\n",
    "Another way of looking at a VAE is as a training procedure for a probablistic model. The model is \n",
    "$$p(x) = \\int p(z)p(x|z) dz$$\n",
    "where $z$ is the latent code and $x$ is the data. During training we need to infer a posterior over $z$. In the case of a VAE this is done by neural network.\n",
    "\n",
    "Assuming that the theory of VAEs has already been presented, we now dive straight into implementing them. If you need more background on VAEs, have a look at our [tutorial slides](https://github.com/philschulz/VITutorial/tree/master/modules) and the references therein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Framework\n",
    "\n",
    "For the purpose of this tutorial we are going to use [mxnet](https://mxnet.incubator.apache.org) which is a scalable deep learning library that has interfaces for several languages, including python. We are going to import and abbreviate it as \"mx\". We will use mxnet to define a computation graph. This is done using the [symbol library](https://mxnet.incubator.apache.org/api/python/symbol.html). When building the VAE, all the methods that you use should be prefixed with `mx.sym`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import urllib.request\n",
    "import os, logging, sys\n",
    "from os.path import join, exists\n",
    "from abc import ABC\n",
    "from typing import List, Tuple, Callable\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify a couple of constants that will help us to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LEARNING_RATE = 0.0003\n",
    "\n",
    "TRAIN_SET = 'train'\n",
    "VALID_SET = 'valid'\n",
    "TEST_SET = 'test'\n",
    "data_names = [TRAIN_SET, VALID_SET, TEST_SET]\n",
    "test_set = [TEST_SET]\n",
    "data_dir = join(os.curdir, \"binary_mnist\")\n",
    "\n",
    "# change this to mx.gpu(0) if you want to run your code on gpu\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up basic logging facilities to print intermediate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s [%(levelname)s]: %(message)s\", datefmt=\"%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "Throughout the tutorial we will use the binarised MNIST data set consisting of images of handwritten digits (0-9). Each pixel has been mapped to either 0 or 1, meaning that pixels are either fully on or off. We use this data set because it allows us to use a rather simple product of Bernoullis as a likelihood. We download the data into a folder called \"binary_mnist\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:12:50 [INFO]: Data file binary_mnist.train exists\n",
      "16:12:50 [INFO]: Data file binary_mnist.valid exists\n",
      "16:12:50 [INFO]: Data file binary_mnist.test exists\n"
     ]
    }
   ],
   "source": [
    "if not exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "for data_set in data_names:\n",
    "    file_name = \"binary_mnist.{}\".format(data_set)\n",
    "    goal = join(data_dir, file_name)\n",
    "    if exists(goal):\n",
    "        logging.info(\"Data file {} exists\".format(file_name))\n",
    "    else:\n",
    "        logging.info(\"Downloading {}\".format(file_name))\n",
    "        link = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_{}.amat\".format(\n",
    "            data_set)\n",
    "        urllib.request.urlretrieve(link, goal)\n",
    "        logging.info(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we have the data on disk. We will load it later for training and testing. But first, we need to build our VAE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagonal Gaussian VAE\n",
    "\n",
    "The most basic VAE model is one where we assume that the latent variable is multiviariate Gaussian. We fix the prior to be standard normal. During inference, we use a multivariate Gaussian variational distribution with diagonal covariance matrix. This means that we are only modelling variance but not covariance (in fact, a k-dimensional Guassian with diagonal covariance has the same density as a product of k independent univariate Gaussians). Geometrically, this variational distribution can only account for spherical but not for elliptical densities. It is thus rather limited in its modelling capabilities. Still, because it uses a neural network under the hood, it is very expressive. \n",
    "\n",
    "In this tutorial, we will model the mist binarised digit data set. Each image is encoded as a 784-dimensional vector. We will model each of these vectors as a product of 784 Bernoullis (of course, there are better models but we want to keep it simple). Our likelihood is thus a product of independent Bernoullis. The resulting model is formally specified as \n",
    "\n",
    "\\begin{align}z \\sim \\mathcal{N}(0,I) && x_i|z \\sim Bernoulli(NN_{\\theta}(z))~~~ i \\in \\{1,2,\\ldots, 784\\} \\ .\\end{align}\n",
    "\n",
    "The variational approximation is given by $$q(z|x) = \\mathcal{N}(NN_{\\lambda}(x), NN_{\\lambda}(x)).$$\n",
    "\n",
    "Notice that both the Bernoulli likelihood and the Gaussian variational distribution use NNs to compute their parameters. The parameters of the NNs, however, are different ($\\theta$ and $\\lambda$, respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We will spread our implementation across 3 classes. This design choice is motivated by the desire to make our models as modular as possible. This will later allow us to mix and match different likelihoods and variational distributions.\n",
    "\n",
    "* **Generator**: This class defines our likelihood. Given a latent value, it produces a data sample our assign a density to an existing data point.\n",
    "* **InferenceNetwork**: This neural network computes the parameters of the variational approximation from a data point.\n",
    "* **VAE**: This is the variational autoencoder. It combines a Generator and an InferenceNetwork and trains them jointly. Once trained, it can generate random data points or try to reproduce data presented to it.\n",
    "\n",
    "Below we have specified these classes abstractly. Make sure you understand what each method is supposed to be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(ABC):\n",
    "    \"\"\"\n",
    "    Generator network.\n",
    "\n",
    "    :param data_dims: Dimensionality of the generated data.\n",
    "    :param layer_sizes: Size of each layer in the network.\n",
    "    :param act_type: The activation after each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dims: int, layer_sizes: List[int], act_type: str) -> None:\n",
    "        self.data_dims = data_dims\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.act_type = act_type\n",
    "\n",
    "    def generate_sample(self, latent_state: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generate a data sample from a latent state.\n",
    "\n",
    "        :param latent_state: The latent input state.\n",
    "        :return: A data sample.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def train(self, latent_state: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Train the generator from a given latent state.\n",
    "        \n",
    "        :param latent_state: The latent input state\n",
    "        :return: The loss symbol used for training\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "class InferenceNetwork(ABC):\n",
    "    \"\"\"\n",
    "    A network to infer distributions over latent states.\n",
    "\n",
    "    :param latent_variable_size: The dimensionality of the latent variable.\n",
    "    :param layer_sizes: Size of each layer in the network.\n",
    "    :param act_type: The activation after each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_variable_size: int, layer_sizes: List[int], act_type: str) -> None:\n",
    "        self.latent_var_size = latent_variable_size\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.act_type = act_type\n",
    "\n",
    "    def inference(self, data: mx.sym.Symbol) -> Tuple[mx.sym.Symbol, ...]:\n",
    "        \"\"\"\n",
    "        Infer the parameters of the distribution over latent values.\n",
    "\n",
    "        :param data: A data sample.\n",
    "        :return: The parameters of the distribution.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "class VAE(ABC):\n",
    "    \"\"\"\n",
    "    A variational autoencoding model (Kingma and Welling, 2013).\n",
    "\n",
    "    :param generator: A generator network that specifies the likelihood of the model.\n",
    "    :param inference_net: An inference network that specifies the distribution over latent values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, generator: Generator, inference_net: InferenceNetwork) -> None:\n",
    "        self.generator = generator\n",
    "        self.inference_net = inference_net\n",
    "\n",
    "    def train(self, data: mx.sym.Symbol, label: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Train the generator and inference network jointly by optimising the ELBO.\n",
    "\n",
    "        :param data: The training data.\n",
    "        :param label: Copy of the training data.\n",
    "        :return: A list of loss symbols.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def generate_reconstructions(self, data: mx.sym.Symbol, n: int) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generate a number of reconstructions of input data points.\n",
    "\n",
    "        :param data: The input data.\n",
    "        :param n: Number of reconstructions per data point.\n",
    "        :return: The reconstructed data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def phantasize(self, n: int) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generate data by randomly sampling from the prior.\n",
    "\n",
    "        :param n: Number of sampled data points.\n",
    "        :return: Randomly generated data points.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Let us start by implementing the generator. This is pretty much a standard neural network. The main point of this exercise is to get you comfortable with mxnet. Complete all the TODOs below. Before starting, check mxnet's [fully connected layer](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.FullyConnected) and [activation functions](https://mxnet.incubator.apache.org/api/python/symbol.html#mxnet.symbol.Activation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductOfBernoullisGenerator(Generator):\n",
    "    \"\"\"\n",
    "    A generator that produces binary vectors whose entries are independent Bernoulli draws.\n",
    "\n",
    "    :param data_dims: Dimensionality of the generated data.\n",
    "    :param layer_sizes: Size of each layer in the network.\n",
    "    :param act_type: The activation after each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dims: int, layer_sizes=List[int], act_type=str) -> None:\n",
    "        super().__init__(data_dims, layer_sizes, act_type)\n",
    "        # TODO choose the correct output activation for a Bernoulli variable. This should just be a string.\n",
    "        self.output_act = \"sigmoid\"\n",
    "\n",
    "    def _preactivation(self, latent_state: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Computes the pre-activation of the generator, i.e. the hidden state before the final output activation.\n",
    "\n",
    "        :param latent_state: The input latent state\n",
    "        :return: The pre-activation before output activation\n",
    "        \"\"\"\n",
    "        prev_out = latent_state\n",
    "        for i, size in enumerate(self.layer_sizes):\n",
    "            fc_i = mx.sym.FullyConnected(data=prev_out, num_hidden=size, name=\"gen_fc_{}\".format(i))\n",
    "            act_i = mx.sym.Activation(data=fc_i, act_type=self.act_type, name=\"gen_act_{}\".format(i))\n",
    "            prev_out = act_i\n",
    "\n",
    "        # The output layer that gives pre_activations for multiple Bernoulli softmax between 0 and 1\n",
    "        fc_out = mx.sym.FullyConnected(data=prev_out, num_hidden=self.data_dims, name=\"gen_fc_out\")\n",
    "\n",
    "        return fc_out\n",
    "    \n",
    "    def generate_sample(self, latent_state: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generates a data sample by picking producing the maximally likely outcome. The stochasticity in the sampling\n",
    "        process comes from the latent_state.\n",
    "\n",
    "        :param latent_state: The input latent state.\n",
    "        :return: A vector of Bernoulli draws.\n",
    "        \"\"\"\n",
    "        act = mx.sym.Activation(data=self._preactivation(latent_state=latent_state), act_type=self.output_act,\n",
    "                                name=\"gen_act_out\")\n",
    "        out = act > 0.5\n",
    "        return out\n",
    "\n",
    "    def train(self, latent_state=mx.sym.Symbol, label=mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Train the generator from a given latent state\n",
    "\n",
    "        :param latent_state: The input latent state\n",
    "        :param label: A binary vector (same as input for inference module)\n",
    "        :return: The loss symbol used for training\n",
    "        \"\"\"\n",
    "        output = mx.sym.Activation(data=self._preactivation(latent_state=latent_state), act_type=self.output_act,\n",
    "                                   name=\"output_act\")\n",
    "        return mx.sym.sum(label * mx.sym.log(output) + (1-label) * mx.sym.log(1-output), axis=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "We now move on to the inference network. Recall that this network will return the parameters of a diagonal Gaussian. Thus, we need to return to vectors of the same size: a mean and a standard deviation vector. (Formally, the parameters of the Gaussian are the variances. However, from the derivation of the Gaussian reparametrisation we know that we\n",
    "need the standard deviations to generate a Gaussian random variable $z$ as transformation of a standard Gaussian variable $\\epsilon$.)\n",
    "\n",
    "**Hint:** In this exercise you will need to draw a random Gaussian sample (see [here](https://mxnet.incubator.apache.org/api/python/symbol.html#mxnet.symbol.random_normal)). The operator requires a\n",
    "shape whose first entry is the batch size. The batch size is not known to you during implementation, however.\n",
    "You can leave it underspecified by choosing $0$ as a value. When you combine the sampling operator with another\n",
    "operator mxnet will infer the correct the batch size for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianInferenceNetwork(InferenceNetwork):\n",
    "    \"\"\"\n",
    "    An inference network that predicts the parameters of a diagonal Gaussian and samples from that distribution.\n",
    "\n",
    "    :param latent_variable_size: The dimensionality of the latent variable.\n",
    "    :param layer_sizes: Size of each layer in the network.\n",
    "    :param act_type: The activation after each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_variable_size: int, layer_sizes: List[int], act_type: str):\n",
    "        super().__init__(latent_variable_size, layer_sizes, act_type)\n",
    "\n",
    "    def inference(self, data: mx.sym.Symbol) -> Tuple[mx.sym.Symbol, mx.sym.Symbol]:\n",
    "        \"\"\"\n",
    "        Infer the mean and standard deviation.\n",
    "\n",
    "        :param data: A data sample.\n",
    "        :return: The mean and standard deviation.\n",
    "        \"\"\"\n",
    "        # We choose to share the first layer between the networks that compute the standard deviations\n",
    "        # and means. This is a fairly standard design choice.\n",
    "        shared_layer = mx.sym.FullyConnected(data=data, num_hidden=self.layer_sizes[0], name=\"inf_joint_fc\")\n",
    "        shared_layer = mx.sym.Activation(data=shared_layer, act_type=self.act_type, name=\"inf_joint_act\")\n",
    "\n",
    "        prev_out = shared_layer\n",
    "        for i, size in enumerate(self.layer_sizes[1:]):\n",
    "            mean_fc_i = mx.sym.FullyConnected(data=prev_out, num_hidden=size, name=\"inf_mean_fc_{}\".format(i))\n",
    "            prev_out = mx.sym.Activation(data=mean_fc_i, act_type=self.act_type, name=\"inf_mean_act_{}\".format(i))\n",
    "        mean = mx.sym.FullyConnected(data=prev_out, num_hidden=self.latent_var_size, name=\"inf_mean_compute\")\n",
    "\n",
    "        prev_out = shared_layer\n",
    "        for i, size in enumerate(self.layer_sizes[1:]):\n",
    "            var_fc_i = mx.sym.FullyConnected(data=prev_out, num_hidden=size, name=\"rec_var_fc_{}\".format(i))\n",
    "            prev_out = mx.sym.Activation(data=var_fc_i, act_type=self.act_type, name=\"rec_var_act_{}\".format(i))\n",
    "        # soft-relu maps std onto non-negative real line\n",
    "        std = mx.sym.Activation(\n",
    "            mx.sym.FullyConnected(data=prev_out, num_hidden=self.latent_var_size, name=\"inf_var_compute\"),\n",
    "            act_type=\"softrelu\")\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "    def sample_latent_state(self, mean: mx.sym.Symbol, std: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Sample a latent Gaussian variable\n",
    "\n",
    "        :param mean: The mean of the Gaussian\n",
    "        :param std: The standard deviation of the Gaussian\n",
    "        :return: A Gaussian sample\n",
    "        \"\"\"\n",
    "        # TODO: This is where the magic happens! Draw a sample from the Gaussian using the Gaussian reparametrisation\n",
    "        # trick and return it.\n",
    "        return mean + std * mx.sym.random_normal(loc=0, scale=1, shape=(0, self.latent_var_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.a\n",
    "\n",
    "Finally, we will put it all together and build our VAE. Recall that the objective for the inference net contains a KL term. You will need to implement that KL-term. Once it is implemented, we can take advantage of autograd to get its gradients. Use the [log](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.log) and [sum](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.sum) symbols here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_gaussian_kl(mean: mx.sym.Symbol, std: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "    var = std ** 2\n",
    "    return -0.5 * mx.sym.sum(data=1 + mx.sym.log(var) - mean ** 2 - var, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3.b\n",
    "\n",
    "The only thing that is left to do is to implement VAE training. To train our VAE we will maximise the ELBO:\n",
    "$$ \\mathbb{E}\\left[ \\log p(x|z) \\right] - \\text{KL}(q(z)||p(z)) \\ . $$\n",
    "Recall that we assume that $ p(z) = \\mathcal{N}(z;0,I) $. Mxnet's optimisers minimise losses instead of maximising objectives. We thus turn the ELBO into a loss by taking its negative. Losses in mxnet are defined using the [MakeLoss](https://mxnet.incubator.apache.org/api/python/symbol.html#mxnet.symbol.MakeLoss) symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianVAE(VAE):\n",
    "    \"\"\"\n",
    "    A VAE with Gaussian latent variables. It assumes a standard normal prior on the latent values.\n",
    "\n",
    "    :param generator: A generator network that specifies the likelihood of the model.\n",
    "    :param inference_net: An inference network that specifies the Gaussian over latent values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 generator: Generator,\n",
    "                 inference_net: GaussianInferenceNetwork,\n",
    "                 kl_divergence: Callable) -> None:\n",
    "        self.generator = generator\n",
    "        self.inference_net = inference_net\n",
    "        self.kl_divergence = kl_divergence\n",
    "\n",
    "    def train(self, data: mx.sym.Symbol, label: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Train the generator and inference network jointly by optimising the ELBO.\n",
    "\n",
    "        :param data: The training data.\n",
    "        :param label: Copy of the training data.\n",
    "        :return: A list of loss symbols.\n",
    "        \"\"\"\n",
    "        mean, std = self.inference_net.inference(data=data)\n",
    "        latent_state = self.inference_net.sample_latent_state(mean, std)\n",
    "        kl_term = self.kl_divergence(mean, std)\n",
    "        log_likelihood = self.generator.train(latent_state=latent_state, label=label)\n",
    "        return mx.sym.MakeLoss(kl_term - log_likelihood)\n",
    "\n",
    "    def generate_reconstructions(self, data: mx.sym.Symbol, n: int) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generate a number of reconstructions of input data points.\n",
    "\n",
    "        :param data: The input data.\n",
    "        :param n: Number of reconstructions per data point.\n",
    "        :return: The reconstructed data.\n",
    "        \"\"\"\n",
    "        mean, std = self.inference_net.inference(data=data)\n",
    "        mean = mx.sym.tile(data=mean, reps=(n, 1))\n",
    "        std = mx.sym.tile(data=std, reps=(n, 1))\n",
    "        latent_state = self.sample_latent_state(mean, std, n)\n",
    "        return self.generator.generate_sample(latent_state=latent_state)\n",
    "\n",
    "    def phantasize(self, n: int) -> mx.sym.Symbol:\n",
    "        \"\"\"\n",
    "        Generate data by randomly sampling from the prior.\n",
    "\n",
    "        :param n: Number of sampled data points.\n",
    "        :return: Randomly generated data points.\n",
    "        \"\"\"\n",
    "        latent_state = mx.sym.random_normal(loc=0, scale=1, shape=(n, self.inference_net.latent_var_size))\n",
    "        return self.generator.generate_sample(latent_state=latent_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a VAE\n",
    "\n",
    "We have now all the code for VAEs in place. Below, we have defined a factory method that makes it easier for you to play with different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_vae(latent_type: str,\n",
    "                  likelihood: str,\n",
    "                  generator_layer_sizes: List[int],\n",
    "                  infer_layer_sizes: List[int],\n",
    "                  latent_variable_size: int,\n",
    "                  data_dims: int,\n",
    "                  generator_act_type: str = \"tanh\",\n",
    "                  infer_act_type: str = \"tanh\") -> VAE:\n",
    "    \"\"\"\n",
    "    Construct a variational autoencoder\n",
    "\n",
    "    :param latent_type: Distribution of latent variable.\n",
    "    :param likelihood: Type of likelihood.\n",
    "    :param generator_layer_sizes: Sizes of generator hidden layers.\n",
    "    :param infer_layer_size: Sizes of inference network hidden layers.\n",
    "    :param latent_variable_size: Size of the latent variable.\n",
    "    :param data_dims: Dimensionality of the data.\n",
    "    :param generator_act_type: Activation function for generator hidden layers.\n",
    "    :param infer_act_type: Activation function for inference network hidden layers.\n",
    "    :return: A variational autoencoder.\n",
    "    \"\"\"\n",
    "    if likelihood == \"bernoulliProd\":\n",
    "        generator = ProductOfBernoullisGenerator(data_dims=data_dims, layer_sizes=generator_layer_sizes,\n",
    "                                                 act_type=generator_act_type)\n",
    "    else:\n",
    "        raise Exception(\"{} is an invalid likelihood type.\".format(likelihood))\n",
    "\n",
    "    if latent_type == \"gaussian\":\n",
    "        inference_net = GaussianInferenceNetwork(latent_variable_size=latent_variable_size,\n",
    "                                                 layer_sizes=infer_layer_sizes,\n",
    "                                                 act_type=infer_act_type)\n",
    "        return GaussianVAE(generator=generator, inference_net=inference_net, kl_divergence=diagonal_gaussian_kl)\n",
    "    else:\n",
    "        raise Exception(\"{} is an invalid latent variable type.\".format(latent_type))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "\n",
    "Your turn! Construct your own VAE below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = construct_vae(latent_type=\"gaussian\", likelihood=\"bernoulliProd\", generator_layer_sizes=[200,500],\n",
    "                   infer_layer_sizes=[500,200], latent_variable_size=200, data_dims=784, generator_act_type=\"tanh\",\n",
    "                   infer_act_type=\"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check what your VAE looks like we will visualise it. The variables are \"data\" and \"label\" are unbound variables in the computation graph. We will shortly use them to supply data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.36.0 (20140111.2315)\n",
       " -->\n",
       "<!-- Title: plot Pages: 1 -->\n",
       "<svg width=\"401pt\" height=\"2040pt\"\n",
       " viewBox=\"0.00 0.00 401.00 2040.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 2036)\">\n",
       "<title>plot</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-2036 397,-2036 397,4 -4,4\"/>\n",
       "<!-- data -->\n",
       "<g id=\"node1\" class=\"node\"><title>data</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"black\" cx=\"144\" cy=\"-29\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"144\" y=\"-25.3\" font-family=\"Times,serif\" font-size=\"14.00\">data</text>\n",
       "</g>\n",
       "<!-- inf_joint_fc -->\n",
       "<g id=\"node2\" class=\"node\"><title>inf_joint_fc</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"191,-152 97,-152 97,-94 191,-94 191,-152\"/>\n",
       "<text text-anchor=\"middle\" x=\"144\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"144\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\">500</text>\n",
       "</g>\n",
       "<!-- inf_joint_fc&#45;&gt;data -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>inf_joint_fc&#45;&gt;data</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144,-83.7443C144,-75.2043 144,-66.2977 144,-58.2479\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144,-93.8971 139.5,-83.897 144,-88.8971 144,-83.8971 144,-83.8971 144,-83.8971 144,-88.8971 148.5,-83.8971 144,-93.8971 144,-93.8971\"/>\n",
       "</g>\n",
       "<!-- inf_joint_act -->\n",
       "<g id=\"node3\" class=\"node\"><title>inf_joint_act</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"191,-246 97,-246 97,-188 191,-188 191,-246\"/>\n",
       "<text text-anchor=\"middle\" x=\"144\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"144\" y=\"-205.8\" font-family=\"Times,serif\" font-size=\"14.00\">tanh</text>\n",
       "</g>\n",
       "<!-- inf_joint_act&#45;&gt;inf_joint_fc -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>inf_joint_act&#45;&gt;inf_joint_fc</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144,-177.744C144,-169.204 144,-160.298 144,-152.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"144,-187.897 139.5,-177.897 144,-182.897 144,-177.897 144,-177.897 144,-177.897 144,-182.897 148.5,-177.897 144,-187.897 144,-187.897\"/>\n",
       "</g>\n",
       "<!-- rec_var_fc_0 -->\n",
       "<g id=\"node4\" class=\"node\"><title>rec_var_fc_0</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"149,-340 55,-340 55,-282 149,-282 149,-340\"/>\n",
       "<text text-anchor=\"middle\" x=\"102\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"102\" y=\"-299.8\" font-family=\"Times,serif\" font-size=\"14.00\">200</text>\n",
       "</g>\n",
       "<!-- rec_var_fc_0&#45;&gt;inf_joint_act -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>rec_var_fc_0&#45;&gt;inf_joint_act</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M119.08,-272.588C123.095,-263.793 127.308,-254.563 131.104,-246.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"114.83,-281.897 114.889,-270.931 116.906,-277.349 118.983,-272.8 118.983,-272.8 118.983,-272.8 116.906,-277.349 123.076,-274.669 114.83,-281.897 114.83,-281.897\"/>\n",
       "</g>\n",
       "<!-- rec_var_act_0 -->\n",
       "<g id=\"node5\" class=\"node\"><title>rec_var_act_0</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"131,-434 37,-434 37,-376 131,-376 131,-434\"/>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-408.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-393.8\" font-family=\"Times,serif\" font-size=\"14.00\">tanh</text>\n",
       "</g>\n",
       "<!-- rec_var_act_0&#45;&gt;rec_var_fc_0 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>rec_var_act_0&#45;&gt;rec_var_fc_0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M91.4298,-366.026C93.1174,-357.4 94.881,-348.386 96.4732,-340.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.4984,-375.897 87.0023,-365.219 90.4585,-370.99 91.4186,-366.083 91.4186,-366.083 91.4186,-366.083 90.4585,-370.99 95.8349,-366.947 89.4984,-375.897 89.4984,-375.897\"/>\n",
       "</g>\n",
       "<!-- inf_var_compute -->\n",
       "<g id=\"node6\" class=\"node\"><title>inf_var_compute</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"131,-528 37,-528 37,-470 131,-470 131,-528\"/>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-502.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-487.8\" font-family=\"Times,serif\" font-size=\"14.00\">200</text>\n",
       "</g>\n",
       "<!-- inf_var_compute&#45;&gt;rec_var_act_0 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>inf_var_compute&#45;&gt;rec_var_act_0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84,-459.744C84,-451.204 84,-442.298 84,-434.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84,-469.897 79.5001,-459.897 84,-464.897 84.0001,-459.897 84.0001,-459.897 84.0001,-459.897 84,-464.897 88.5001,-459.897 84,-469.897 84,-469.897\"/>\n",
       "</g>\n",
       "<!-- activation9 -->\n",
       "<g id=\"node7\" class=\"node\"><title>activation9</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"131,-622 37,-622 37,-564 131,-564 131,-622\"/>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-596.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-581.8\" font-family=\"Times,serif\" font-size=\"14.00\">softrelu</text>\n",
       "</g>\n",
       "<!-- activation9&#45;&gt;inf_var_compute -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>activation9&#45;&gt;inf_var_compute</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84,-553.744C84,-545.204 84,-536.298 84,-528.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84,-563.897 79.5001,-553.897 84,-558.897 84.0001,-553.897 84.0001,-553.897 84.0001,-553.897 84,-558.897 88.5001,-553.897 84,-563.897 84,-563.897\"/>\n",
       "</g>\n",
       "<!-- _powerscalar18 -->\n",
       "<g id=\"node8\" class=\"node\"><title>_powerscalar18</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"131,-716 37,-716 37,-658 131,-658 131,-716\"/>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-683.3\" font-family=\"Times,serif\" font-size=\"14.00\">_powerscalar18</text>\n",
       "</g>\n",
       "<!-- _powerscalar18&#45;&gt;activation9 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>_powerscalar18&#45;&gt;activation9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84,-647.744C84,-639.204 84,-630.298 84,-622.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84,-657.897 79.5001,-647.897 84,-652.897 84.0001,-647.897 84.0001,-647.897 84.0001,-647.897 84,-652.897 88.5001,-647.897 84,-657.897 84,-657.897\"/>\n",
       "</g>\n",
       "<!-- log27 -->\n",
       "<g id=\"node9\" class=\"node\"><title>log27</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"131,-810 37,-810 37,-752 131,-752 131,-810\"/>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-777.3\" font-family=\"Times,serif\" font-size=\"14.00\">log27</text>\n",
       "</g>\n",
       "<!-- log27&#45;&gt;_powerscalar18 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>log27&#45;&gt;_powerscalar18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84,-741.744C84,-733.204 84,-724.298 84,-716.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84,-751.897 79.5001,-741.897 84,-746.897 84.0001,-741.897 84.0001,-741.897 84.0001,-741.897 84,-746.897 88.5001,-741.897 84,-751.897 84,-751.897\"/>\n",
       "</g>\n",
       "<!-- _plusscalar9 -->\n",
       "<g id=\"node10\" class=\"node\"><title>_plusscalar9</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"131,-904 37,-904 37,-846 131,-846 131,-904\"/>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-871.3\" font-family=\"Times,serif\" font-size=\"14.00\">_plusscalar9</text>\n",
       "</g>\n",
       "<!-- _plusscalar9&#45;&gt;log27 -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>_plusscalar9&#45;&gt;log27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84,-835.744C84,-827.204 84,-818.298 84,-810.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84,-845.897 79.5001,-835.897 84,-840.897 84.0001,-835.897 84.0001,-835.897 84.0001,-835.897 84,-840.897 88.5001,-835.897 84,-845.897 84,-845.897\"/>\n",
       "</g>\n",
       "<!-- inf_mean_fc_0 -->\n",
       "<g id=\"node11\" class=\"node\"><title>inf_mean_fc_0</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"243,-434 149,-434 149,-376 243,-376 243,-434\"/>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-408.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-393.8\" font-family=\"Times,serif\" font-size=\"14.00\">200</text>\n",
       "</g>\n",
       "<!-- inf_mean_fc_0&#45;&gt;inf_joint_act -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>inf_mean_fc_0&#45;&gt;inf_joint_act</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M185.305,-365.746C175.369,-330.206 160.803,-278.104 151.887,-246.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"188.102,-375.751 181.076,-367.332 186.756,-370.935 185.41,-366.12 185.41,-366.12 185.41,-366.12 186.756,-370.935 189.744,-364.908 188.102,-375.751 188.102,-375.751\"/>\n",
       "</g>\n",
       "<!-- inf_mean_act_0 -->\n",
       "<g id=\"node12\" class=\"node\"><title>inf_mean_act_0</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"243,-622 149,-622 149,-564 243,-564 243,-622\"/>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-596.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-581.8\" font-family=\"Times,serif\" font-size=\"14.00\">tanh</text>\n",
       "</g>\n",
       "<!-- inf_mean_act_0&#45;&gt;inf_mean_fc_0 -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>inf_mean_act_0&#45;&gt;inf_mean_fc_0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196,-553.746C196,-518.206 196,-466.104 196,-434.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196,-563.751 191.5,-553.751 196,-558.751 196,-553.751 196,-553.751 196,-553.751 196,-558.751 200.5,-553.751 196,-563.751 196,-563.751\"/>\n",
       "</g>\n",
       "<!-- inf_mean_compute -->\n",
       "<g id=\"node13\" class=\"node\"><title>inf_mean_compute</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"243,-716 149,-716 149,-658 243,-658 243,-716\"/>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-690.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-675.8\" font-family=\"Times,serif\" font-size=\"14.00\">200</text>\n",
       "</g>\n",
       "<!-- inf_mean_compute&#45;&gt;inf_mean_act_0 -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>inf_mean_compute&#45;&gt;inf_mean_act_0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196,-647.744C196,-639.204 196,-630.298 196,-622.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196,-657.897 191.5,-647.897 196,-652.897 196,-647.897 196,-647.897 196,-647.897 196,-652.897 200.5,-647.897 196,-657.897 196,-657.897\"/>\n",
       "</g>\n",
       "<!-- _powerscalar19 -->\n",
       "<g id=\"node14\" class=\"node\"><title>_powerscalar19</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"243,-810 149,-810 149,-752 243,-752 243,-810\"/>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-777.3\" font-family=\"Times,serif\" font-size=\"14.00\">_powerscalar19</text>\n",
       "</g>\n",
       "<!-- _powerscalar19&#45;&gt;inf_mean_compute -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>_powerscalar19&#45;&gt;inf_mean_compute</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196,-741.744C196,-733.204 196,-724.298 196,-716.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196,-751.897 191.5,-741.897 196,-746.897 196,-741.897 196,-741.897 196,-741.897 196,-746.897 200.5,-741.897 196,-751.897 196,-751.897\"/>\n",
       "</g>\n",
       "<!-- _minus27 -->\n",
       "<g id=\"node15\" class=\"node\"><title>_minus27</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"131,-998 37,-998 37,-940 131,-940 131,-998\"/>\n",
       "<text text-anchor=\"middle\" x=\"84\" y=\"-965.3\" font-family=\"Times,serif\" font-size=\"14.00\">_minus27</text>\n",
       "</g>\n",
       "<!-- _minus27&#45;&gt;_plusscalar9 -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>_minus27&#45;&gt;_plusscalar9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84,-929.744C84,-921.204 84,-912.298 84,-904.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"84,-939.897 79.5001,-929.897 84,-934.897 84.0001,-929.897 84.0001,-929.897 84.0001,-929.897 84,-934.897 88.5001,-929.897 84,-939.897 84,-939.897\"/>\n",
       "</g>\n",
       "<!-- _minus27&#45;&gt;_powerscalar19 -->\n",
       "<g id=\"edge15\" class=\"edge\"><title>_minus27&#45;&gt;_powerscalar19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118.46,-932.264C126.075,-923.413 133.732,-913.681 140,-904 159.506,-873.873 175.809,-835.844 185.704,-810.321\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"111.815,-939.791 115.06,-929.316 115.124,-936.043 118.433,-932.294 118.433,-932.294 118.433,-932.294 115.124,-936.043 121.807,-935.273 111.815,-939.791 111.815,-939.791\"/>\n",
       "</g>\n",
       "<!-- _minus28 -->\n",
       "<g id=\"node16\" class=\"node\"><title>_minus28</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"94,-1092 -7.10543e-15,-1092 -7.10543e-15,-1034 94,-1034 94,-1092\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-1059.3\" font-family=\"Times,serif\" font-size=\"14.00\">_minus28</text>\n",
       "</g>\n",
       "<!-- _minus28&#45;&gt;_powerscalar18 -->\n",
       "<g id=\"edge17\" class=\"edge\"><title>_minus28&#45;&gt;_powerscalar18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M33.7872,-1024.08C31.384,-1015.6 29.2458,-1006.57 28,-998 12.2786,-889.803 -10.4554,-854.347 28,-752 32.9852,-738.732 42.1065,-726.415 51.5608,-716.202\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"36.6926,-1033.72 29.4994,-1025.44 35.2504,-1028.93 33.8082,-1024.15 33.8082,-1024.15 33.8082,-1024.15 35.2504,-1028.93 38.117,-1022.85 36.6926,-1033.72 36.6926,-1033.72\"/>\n",
       "</g>\n",
       "<!-- _minus28&#45;&gt;_minus27 -->\n",
       "<g id=\"edge16\" class=\"edge\"><title>_minus28&#45;&gt;_minus27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M62.0463,-1024.59C65.5834,-1015.79 69.2953,-1006.56 72.6395,-998.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"58.3023,-1033.9 57.8586,-1022.94 60.1679,-1029.26 62.0336,-1024.62 62.0336,-1024.62 62.0336,-1024.62 60.1679,-1029.26 66.2086,-1026.3 58.3023,-1033.9 58.3023,-1033.9\"/>\n",
       "</g>\n",
       "<!-- sum18 -->\n",
       "<g id=\"node17\" class=\"node\"><title>sum18</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"113,-1750 19,-1750 19,-1692 113,-1692 113,-1750\"/>\n",
       "<text text-anchor=\"middle\" x=\"66\" y=\"-1717.3\" font-family=\"Times,serif\" font-size=\"14.00\">sum18</text>\n",
       "</g>\n",
       "<!-- sum18&#45;&gt;_minus28 -->\n",
       "<g id=\"edge18\" class=\"edge\"><title>sum18&#45;&gt;_minus28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.7927,-1681.74C54.2497,-1644.22 47,-1585.33 47,-1534 47,-1534 47,-1534 47,-1250 47,-1193.97 47,-1128.46 47,-1092.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"61.3034,-1691.73 55.359,-1682.52 60.5559,-1686.79 59.8085,-1681.84 59.8085,-1681.84 59.8085,-1681.84 60.5559,-1686.79 64.2579,-1681.17 61.3034,-1691.73 61.3034,-1691.73\"/>\n",
       "</g>\n",
       "<!-- _mulscalar9 -->\n",
       "<g id=\"node18\" class=\"node\"><title>_mulscalar9</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"150,-1844 56,-1844 56,-1786 150,-1786 150,-1844\"/>\n",
       "<text text-anchor=\"middle\" x=\"103\" y=\"-1811.3\" font-family=\"Times,serif\" font-size=\"14.00\">_mulscalar9</text>\n",
       "</g>\n",
       "<!-- _mulscalar9&#45;&gt;sum18 -->\n",
       "<g id=\"edge19\" class=\"edge\"><title>_mulscalar9&#45;&gt;sum18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M87.9537,-1776.59C84.4166,-1767.79 80.7047,-1758.56 77.3605,-1750.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"91.6977,-1785.9 83.7914,-1778.3 89.8321,-1781.26 87.9664,-1776.62 87.9664,-1776.62 87.9664,-1776.62 89.8321,-1781.26 92.1414,-1774.94 91.6977,-1785.9 91.6977,-1785.9\"/>\n",
       "</g>\n",
       "<!-- label -->\n",
       "<g id=\"node19\" class=\"node\"><title>label</title>\n",
       "<ellipse fill=\"#8dd3c7\" stroke=\"black\" cx=\"234\" cy=\"-1439\" rx=\"47\" ry=\"29\"/>\n",
       "<text text-anchor=\"middle\" x=\"234\" y=\"-1435.3\" font-family=\"Times,serif\" font-size=\"14.00\">label</text>\n",
       "</g>\n",
       "<!-- random_normal18 -->\n",
       "<g id=\"node20\" class=\"node\"><title>random_normal18</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"355,-622 261,-622 261,-564 355,-564 355,-622\"/>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-589.3\" font-family=\"Times,serif\" font-size=\"14.00\">random_normal18</text>\n",
       "</g>\n",
       "<!-- _mul27 -->\n",
       "<g id=\"node21\" class=\"node\"><title>_mul27</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"355,-716 261,-716 261,-658 355,-658 355,-716\"/>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-683.3\" font-family=\"Times,serif\" font-size=\"14.00\">_mul27</text>\n",
       "</g>\n",
       "<!-- _mul27&#45;&gt;activation9 -->\n",
       "<g id=\"edge20\" class=\"edge\"><title>_mul27&#45;&gt;activation9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M251.535,-658.054C201.762,-638.573 185.671,-640.63 140,-622 137.081,-620.809 134.105,-619.529 131.122,-618.195\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260.878,-661.805 249.921,-662.255 256.238,-659.942 251.598,-658.079 251.598,-658.079 251.598,-658.079 256.238,-659.942 253.274,-653.903 260.878,-661.805 260.878,-661.805\"/>\n",
       "</g>\n",
       "<!-- _mul27&#45;&gt;random_normal18 -->\n",
       "<g id=\"edge21\" class=\"edge\"><title>_mul27&#45;&gt;random_normal18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M308,-647.744C308,-639.204 308,-630.298 308,-622.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308,-657.897 303.5,-647.897 308,-652.897 308,-647.897 308,-647.897 308,-647.897 308,-652.897 312.5,-647.897 308,-657.897 308,-657.897\"/>\n",
       "</g>\n",
       "<!-- _plus18 -->\n",
       "<g id=\"node22\" class=\"node\"><title>_plus18</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"355,-810 261,-810 261,-752 355,-752 355,-810\"/>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-777.3\" font-family=\"Times,serif\" font-size=\"14.00\">_plus18</text>\n",
       "</g>\n",
       "<!-- _plus18&#45;&gt;inf_mean_compute -->\n",
       "<g id=\"edge22\" class=\"edge\"><title>_plus18&#45;&gt;inf_mean_compute</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M265.852,-745.378C254.151,-735.767 241.589,-725.448 230.389,-716.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"273.788,-751.897 263.204,-749.027 269.924,-748.723 266.06,-745.55 266.06,-745.55 266.06,-745.55 269.924,-748.723 268.917,-742.072 273.788,-751.897 273.788,-751.897\"/>\n",
       "</g>\n",
       "<!-- _plus18&#45;&gt;_mul27 -->\n",
       "<g id=\"edge23\" class=\"edge\"><title>_plus18&#45;&gt;_mul27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M308,-741.744C308,-733.204 308,-724.298 308,-716.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308,-751.897 303.5,-741.897 308,-746.897 308,-741.897 308,-741.897 308,-741.897 308,-746.897 312.5,-741.897 308,-751.897 308,-751.897\"/>\n",
       "</g>\n",
       "<!-- gen_fc_0 -->\n",
       "<g id=\"node23\" class=\"node\"><title>gen_fc_0</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"355,-904 261,-904 261,-846 355,-846 355,-904\"/>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-878.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-863.8\" font-family=\"Times,serif\" font-size=\"14.00\">200</text>\n",
       "</g>\n",
       "<!-- gen_fc_0&#45;&gt;_plus18 -->\n",
       "<g id=\"edge24\" class=\"edge\"><title>gen_fc_0&#45;&gt;_plus18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M308,-835.744C308,-827.204 308,-818.298 308,-810.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308,-845.897 303.5,-835.897 308,-840.897 308,-835.897 308,-835.897 308,-835.897 308,-840.897 312.5,-835.897 308,-845.897 308,-845.897\"/>\n",
       "</g>\n",
       "<!-- gen_act_0 -->\n",
       "<g id=\"node24\" class=\"node\"><title>gen_act_0</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"355,-998 261,-998 261,-940 355,-940 355,-998\"/>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-972.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-957.8\" font-family=\"Times,serif\" font-size=\"14.00\">tanh</text>\n",
       "</g>\n",
       "<!-- gen_act_0&#45;&gt;gen_fc_0 -->\n",
       "<g id=\"edge25\" class=\"edge\"><title>gen_act_0&#45;&gt;gen_fc_0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M308,-929.744C308,-921.204 308,-912.298 308,-904.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308,-939.897 303.5,-929.897 308,-934.897 308,-929.897 308,-929.897 308,-929.897 308,-934.897 312.5,-929.897 308,-939.897 308,-939.897\"/>\n",
       "</g>\n",
       "<!-- gen_fc_1 -->\n",
       "<g id=\"node25\" class=\"node\"><title>gen_fc_1</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"355,-1092 261,-1092 261,-1034 355,-1034 355,-1092\"/>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-1066.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-1051.8\" font-family=\"Times,serif\" font-size=\"14.00\">500</text>\n",
       "</g>\n",
       "<!-- gen_fc_1&#45;&gt;gen_act_0 -->\n",
       "<g id=\"edge26\" class=\"edge\"><title>gen_fc_1&#45;&gt;gen_act_0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M308,-1023.74C308,-1015.2 308,-1006.3 308,-998.248\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308,-1033.9 303.5,-1023.9 308,-1028.9 308,-1023.9 308,-1023.9 308,-1023.9 308,-1028.9 312.5,-1023.9 308,-1033.9 308,-1033.9\"/>\n",
       "</g>\n",
       "<!-- gen_act_1 -->\n",
       "<g id=\"node26\" class=\"node\"><title>gen_act_1</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"355,-1186 261,-1186 261,-1128 355,-1128 355,-1186\"/>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-1160.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-1145.8\" font-family=\"Times,serif\" font-size=\"14.00\">tanh</text>\n",
       "</g>\n",
       "<!-- gen_act_1&#45;&gt;gen_fc_1 -->\n",
       "<g id=\"edge27\" class=\"edge\"><title>gen_act_1&#45;&gt;gen_fc_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M308,-1117.74C308,-1109.2 308,-1100.3 308,-1092.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308,-1127.9 303.5,-1117.9 308,-1122.9 308,-1117.9 308,-1117.9 308,-1117.9 308,-1122.9 312.5,-1117.9 308,-1127.9 308,-1127.9\"/>\n",
       "</g>\n",
       "<!-- gen_fc_out -->\n",
       "<g id=\"node27\" class=\"node\"><title>gen_fc_out</title>\n",
       "<polygon fill=\"#fb8072\" stroke=\"black\" points=\"355,-1280 261,-1280 261,-1222 355,-1222 355,-1280\"/>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-1254.8\" font-family=\"Times,serif\" font-size=\"14.00\">FullyConnected</text>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-1239.8\" font-family=\"Times,serif\" font-size=\"14.00\">784</text>\n",
       "</g>\n",
       "<!-- gen_fc_out&#45;&gt;gen_act_1 -->\n",
       "<g id=\"edge28\" class=\"edge\"><title>gen_fc_out&#45;&gt;gen_act_1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M308,-1211.74C308,-1203.2 308,-1194.3 308,-1186.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308,-1221.9 303.5,-1211.9 308,-1216.9 308,-1211.9 308,-1211.9 308,-1211.9 308,-1216.9 312.5,-1211.9 308,-1221.9 308,-1221.9\"/>\n",
       "</g>\n",
       "<!-- output_act -->\n",
       "<g id=\"node28\" class=\"node\"><title>output_act</title>\n",
       "<polygon fill=\"#ffffb3\" stroke=\"black\" points=\"355,-1374 261,-1374 261,-1316 355,-1316 355,-1374\"/>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-1348.8\" font-family=\"Times,serif\" font-size=\"14.00\">Activation</text>\n",
       "<text text-anchor=\"middle\" x=\"308\" y=\"-1333.8\" font-family=\"Times,serif\" font-size=\"14.00\">sigmoid</text>\n",
       "</g>\n",
       "<!-- output_act&#45;&gt;gen_fc_out -->\n",
       "<g id=\"edge29\" class=\"edge\"><title>output_act&#45;&gt;gen_fc_out</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M308,-1305.74C308,-1297.2 308,-1288.3 308,-1280.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308,-1315.9 303.5,-1305.9 308,-1310.9 308,-1305.9 308,-1305.9 308,-1305.9 308,-1310.9 312.5,-1305.9 308,-1315.9 308,-1315.9\"/>\n",
       "</g>\n",
       "<!-- log28 -->\n",
       "<g id=\"node29\" class=\"node\"><title>log28</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"393,-1468 299,-1468 299,-1410 393,-1410 393,-1468\"/>\n",
       "<text text-anchor=\"middle\" x=\"346\" y=\"-1435.3\" font-family=\"Times,serif\" font-size=\"14.00\">log28</text>\n",
       "</g>\n",
       "<!-- log28&#45;&gt;output_act -->\n",
       "<g id=\"edge30\" class=\"edge\"><title>log28&#45;&gt;output_act</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M330.547,-1400.59C326.914,-1391.79 323.102,-1382.56 319.668,-1374.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"334.392,-1409.9 326.415,-1402.37 332.483,-1405.28 330.575,-1400.65 330.575,-1400.65 330.575,-1400.65 332.483,-1405.28 334.734,-1398.94 334.392,-1409.9 334.392,-1409.9\"/>\n",
       "</g>\n",
       "<!-- _mul28 -->\n",
       "<g id=\"node30\" class=\"node\"><title>_mul28</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"393,-1562 299,-1562 299,-1504 393,-1504 393,-1562\"/>\n",
       "<text text-anchor=\"middle\" x=\"346\" y=\"-1529.3\" font-family=\"Times,serif\" font-size=\"14.00\">_mul28</text>\n",
       "</g>\n",
       "<!-- _mul28&#45;&gt;label -->\n",
       "<g id=\"edge31\" class=\"edge\"><title>_mul28&#45;&gt;label</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M303.672,-1497.23C289.597,-1485.67 274.296,-1473.1 261.627,-1462.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"311.788,-1503.9 301.204,-1501.03 307.924,-1500.72 304.06,-1497.55 304.06,-1497.55 304.06,-1497.55 307.924,-1500.72 306.917,-1494.07 311.788,-1503.9 311.788,-1503.9\"/>\n",
       "</g>\n",
       "<!-- _mul28&#45;&gt;log28 -->\n",
       "<g id=\"edge32\" class=\"edge\"><title>_mul28&#45;&gt;log28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M346,-1493.74C346,-1485.2 346,-1476.3 346,-1468.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"346,-1503.9 341.5,-1493.9 346,-1498.9 346,-1493.9 346,-1493.9 346,-1493.9 346,-1498.9 350.5,-1493.9 346,-1503.9 346,-1503.9\"/>\n",
       "</g>\n",
       "<!-- _rminusscalar18 -->\n",
       "<g id=\"node31\" class=\"node\"><title>_rminusscalar18</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"281,-1562 187,-1562 187,-1504 281,-1504 281,-1562\"/>\n",
       "<text text-anchor=\"middle\" x=\"234\" y=\"-1529.3\" font-family=\"Times,serif\" font-size=\"14.00\">_rminusscalar18</text>\n",
       "</g>\n",
       "<!-- _rminusscalar18&#45;&gt;label -->\n",
       "<g id=\"edge33\" class=\"edge\"><title>_rminusscalar18&#45;&gt;label</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M234,-1493.74C234,-1485.2 234,-1476.3 234,-1468.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"234,-1503.9 229.5,-1493.9 234,-1498.9 234,-1493.9 234,-1493.9 234,-1493.9 234,-1498.9 238.5,-1493.9 234,-1503.9 234,-1503.9\"/>\n",
       "</g>\n",
       "<!-- _rminusscalar19 -->\n",
       "<g id=\"node32\" class=\"node\"><title>_rminusscalar19</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"169,-1468 75,-1468 75,-1410 169,-1410 169,-1468\"/>\n",
       "<text text-anchor=\"middle\" x=\"122\" y=\"-1435.3\" font-family=\"Times,serif\" font-size=\"14.00\">_rminusscalar19</text>\n",
       "</g>\n",
       "<!-- _rminusscalar19&#45;&gt;output_act -->\n",
       "<g id=\"edge34\" class=\"edge\"><title>_rminusscalar19&#45;&gt;output_act</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177.726,-1410.14C177.817,-1410.09 177.909,-1410.05 178,-1410 205.38,-1396.42 236.054,-1381.31 260.727,-1369.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.043,-1414.45 175.995,-1405.97 173.521,-1412.23 177.998,-1410 177.998,-1410 177.998,-1410 173.521,-1412.23 180.001,-1414.03 169.043,-1414.45 169.043,-1414.45\"/>\n",
       "</g>\n",
       "<!-- log29 -->\n",
       "<g id=\"node33\" class=\"node\"><title>log29</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"169,-1562 75,-1562 75,-1504 169,-1504 169,-1562\"/>\n",
       "<text text-anchor=\"middle\" x=\"122\" y=\"-1529.3\" font-family=\"Times,serif\" font-size=\"14.00\">log29</text>\n",
       "</g>\n",
       "<!-- log29&#45;&gt;_rminusscalar19 -->\n",
       "<g id=\"edge35\" class=\"edge\"><title>log29&#45;&gt;_rminusscalar19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M122,-1493.74C122,-1485.2 122,-1476.3 122,-1468.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122,-1503.9 117.5,-1493.9 122,-1498.9 122,-1493.9 122,-1493.9 122,-1493.9 122,-1498.9 126.5,-1493.9 122,-1503.9 122,-1503.9\"/>\n",
       "</g>\n",
       "<!-- _mul29 -->\n",
       "<g id=\"node34\" class=\"node\"><title>_mul29</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"262,-1656 168,-1656 168,-1598 262,-1598 262,-1656\"/>\n",
       "<text text-anchor=\"middle\" x=\"215\" y=\"-1623.3\" font-family=\"Times,serif\" font-size=\"14.00\">_mul29</text>\n",
       "</g>\n",
       "<!-- _mul29&#45;&gt;_rminusscalar18 -->\n",
       "<g id=\"edge36\" class=\"edge\"><title>_mul29&#45;&gt;_rminusscalar18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.843,-1588.03C224.624,-1579.4 226.485,-1570.39 228.166,-1562.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"220.804,-1597.9 218.419,-1587.19 221.815,-1593 222.826,-1588.1 222.826,-1588.1 222.826,-1588.1 221.815,-1593 227.233,-1589.01 220.804,-1597.9 220.804,-1597.9\"/>\n",
       "</g>\n",
       "<!-- _mul29&#45;&gt;log29 -->\n",
       "<g id=\"edge37\" class=\"edge\"><title>_mul29&#45;&gt;log29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M179.441,-1590.82C169.885,-1581.37 159.676,-1571.27 150.555,-1562.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"186.592,-1597.9 176.318,-1594.06 183.037,-1594.38 179.482,-1590.86 179.482,-1590.86 179.482,-1590.86 183.037,-1594.38 182.647,-1587.67 186.592,-1597.9 186.592,-1597.9\"/>\n",
       "</g>\n",
       "<!-- _plus19 -->\n",
       "<g id=\"node35\" class=\"node\"><title>_plus19</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"262,-1750 168,-1750 168,-1692 262,-1692 262,-1750\"/>\n",
       "<text text-anchor=\"middle\" x=\"215\" y=\"-1717.3\" font-family=\"Times,serif\" font-size=\"14.00\">_plus19</text>\n",
       "</g>\n",
       "<!-- _plus19&#45;&gt;_mul28 -->\n",
       "<g id=\"edge38\" class=\"edge\"><title>_plus19&#45;&gt;_mul28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M248.176,-1684.21C255.966,-1675.24 264.023,-1665.47 271,-1656 293.565,-1625.38 315.947,-1587.78 330.305,-1562.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"241.433,-1691.86 244.669,-1681.39 244.739,-1688.11 248.045,-1684.36 248.045,-1684.36 248.045,-1684.36 244.739,-1688.11 251.421,-1687.34 241.433,-1691.86 241.433,-1691.86\"/>\n",
       "</g>\n",
       "<!-- _plus19&#45;&gt;_mul29 -->\n",
       "<g id=\"edge39\" class=\"edge\"><title>_plus19&#45;&gt;_mul29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215,-1681.74C215,-1673.2 215,-1664.3 215,-1656.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215,-1691.9 210.5,-1681.9 215,-1686.9 215,-1681.9 215,-1681.9 215,-1681.9 215,-1686.9 219.5,-1681.9 215,-1691.9 215,-1691.9\"/>\n",
       "</g>\n",
       "<!-- sum19 -->\n",
       "<g id=\"node36\" class=\"node\"><title>sum19</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"262,-1844 168,-1844 168,-1786 262,-1786 262,-1844\"/>\n",
       "<text text-anchor=\"middle\" x=\"215\" y=\"-1811.3\" font-family=\"Times,serif\" font-size=\"14.00\">sum19</text>\n",
       "</g>\n",
       "<!-- sum19&#45;&gt;_plus19 -->\n",
       "<g id=\"edge40\" class=\"edge\"><title>sum19&#45;&gt;_plus19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M215,-1775.74C215,-1767.2 215,-1758.3 215,-1750.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215,-1785.9 210.5,-1775.9 215,-1780.9 215,-1775.9 215,-1775.9 215,-1775.9 215,-1780.9 219.5,-1775.9 215,-1785.9 215,-1785.9\"/>\n",
       "</g>\n",
       "<!-- _minus29 -->\n",
       "<g id=\"node37\" class=\"node\"><title>_minus29</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"206,-1938 112,-1938 112,-1880 206,-1880 206,-1938\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-1905.3\" font-family=\"Times,serif\" font-size=\"14.00\">_minus29</text>\n",
       "</g>\n",
       "<!-- _minus29&#45;&gt;_mulscalar9 -->\n",
       "<g id=\"edge41\" class=\"edge\"><title>_minus29&#45;&gt;_mulscalar9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M136.569,-1871.15C131.113,-1862.19 125.363,-1852.74 120.194,-1844.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.894,-1879.9 132.85,-1873.69 139.294,-1875.63 136.694,-1871.36 136.694,-1871.36 136.694,-1871.36 139.294,-1875.63 140.538,-1869.02 141.894,-1879.9 141.894,-1879.9\"/>\n",
       "</g>\n",
       "<!-- _minus29&#45;&gt;sum19 -->\n",
       "<g id=\"edge42\" class=\"edge\"><title>_minus29&#45;&gt;sum19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M181.431,-1871.15C186.887,-1862.19 192.637,-1852.74 197.806,-1844.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"176.106,-1879.9 177.462,-1869.02 178.706,-1875.63 181.306,-1871.36 181.306,-1871.36 181.306,-1871.36 178.706,-1875.63 185.15,-1873.69 176.106,-1879.9 176.106,-1879.9\"/>\n",
       "</g>\n",
       "<!-- makeloss9 -->\n",
       "<g id=\"node38\" class=\"node\"><title>makeloss9</title>\n",
       "<polygon fill=\"#fccde5\" stroke=\"black\" points=\"206,-2032 112,-2032 112,-1974 206,-1974 206,-2032\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-1999.3\" font-family=\"Times,serif\" font-size=\"14.00\">makeloss9</text>\n",
       "</g>\n",
       "<!-- makeloss9&#45;&gt;_minus29 -->\n",
       "<g id=\"edge43\" class=\"edge\"><title>makeloss9&#45;&gt;_minus29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-1963.74C159,-1955.2 159,-1946.3 159,-1938.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"159,-1973.9 154.5,-1963.9 159,-1968.9 159,-1963.9 159,-1963.9 159,-1963.9 159,-1968.9 163.5,-1963.9 159,-1973.9 159,-1973.9\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x15db83908>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = mx.sym.Variable(\"data\")\n",
    "label = mx.sym.Variable(\"label\")\n",
    "mx.viz.plot_network(vae.train(data, label))\n",
    "# Comment the line above and uncomment the one below if you want to save the VAE picture on disk\n",
    "#mx.viz.plot_network(vae.train(data, label), title=\"my_vae\", save_format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a VAE\n",
    "\n",
    "We are now set to train the VAE. In order to do so we have to define a data iterator and a module in mxnet. The data iterator takes care of batching the data while the module executes the computation graph and updates the model parameters. For the purpose of training the model, we only need the training data which we load from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:19:32 [INFO]: Reading ./binary_mnist/binary_mnist.train into memory\n",
      "16:20:06 [INFO]: ./binary_mnist/binary_mnist.train contains 50000 data points\n"
     ]
    }
   ],
   "source": [
    "mnist = {}\n",
    "file_name = join(data_dir, \"binary_mnist.{}\".format(TRAIN_SET))\n",
    "logging.info(\"Reading {} into memory\".format(file_name))\n",
    "mnist[TRAIN_SET] = mx.nd.array(genfromtxt(file_name))\n",
    "logging.info(\"{} contains {} data points\".format(file_name, mnist[TRAIN_SET].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that data we define the training set iterator. At this point we also need to choose a batch_size that will then be used in training. Notice that we randomise the order in which the data points are presented in the training set. It is important that `data_name` and `label_name` match the names of the unbound variables in our model. Mxnet will use this information to pass the correct data points to the variables. Finally, notice that the data and labels are identical. This is because the VAE tries to reconstruct its input data and thus labels and data are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "train_iter = mx.io.NDArrayIter(data=mnist[TRAIN_SET], data_name=\"data\", label=mnist[TRAIN_SET], label_name=\"label\",\n",
    "                                   batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a module that will do the training for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_module = mx.module.Module(vae.train(data=mx.sym.Variable(\"data\"), label=mx.sym.Variable(\"label\")),\n",
    "                           data_names=[train_iter.provide_data[0][0]],\n",
    "                           label_names=[train_iter.provide_label[0][0]], context=ctx, logger=logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the vae (or any other network defined in mxnet) is most easily done using the fit method. Here we choose to train our model for 20 epochs. Our optimiser will be adam. Training will take some time (2-5 minutes depending on your machine). We are keeping track of the loss (the negative ELBO) to see how the model develops. Notice that this loss\n",
    "is not the actual negative ELBO but a _doubly stochastic approximation_ to it (see [here](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586) and [here](http://proceedings.mlr.press/v32/titsias14.pdf)). The two sources of stochasticity are the mini-batches (the ELBO is defined with respect to the entire data set) and the reparametrisation trick (we approximate the integral over $ z $ through sampling). Both sources of stochasticity leave the approximation unbiased, however. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:21:09 [INFO]: Epoch[0] Batch [20]\tSpeed: 5098.09 samples/sec\tloss=534.925082\n",
      "16:21:09 [INFO]: Epoch[0] Batch [40]\tSpeed: 5273.32 samples/sec\tloss=284.297896\n",
      "16:21:10 [INFO]: Epoch[0] Batch [60]\tSpeed: 5261.32 samples/sec\tloss=221.287396\n",
      "16:21:11 [INFO]: Epoch[0] Batch [80]\tSpeed: 4977.73 samples/sec\tloss=216.021719\n",
      "16:21:12 [INFO]: Epoch[0] Batch [100]\tSpeed: 5044.54 samples/sec\tloss=215.614590\n",
      "16:21:12 [INFO]: Epoch[0] Batch [120]\tSpeed: 5521.78 samples/sec\tloss=213.830873\n",
      "16:21:13 [INFO]: Epoch[0] Batch [140]\tSpeed: 5803.54 samples/sec\tloss=214.044724\n",
      "16:21:14 [INFO]: Epoch[0] Batch [160]\tSpeed: 5909.85 samples/sec\tloss=214.922380\n",
      "16:21:15 [INFO]: Epoch[0] Batch [180]\tSpeed: 5904.75 samples/sec\tloss=213.873189\n",
      "16:21:15 [INFO]: Epoch[0] Batch [200]\tSpeed: 5907.99 samples/sec\tloss=211.652762\n",
      "16:21:16 [INFO]: Epoch[0] Batch [220]\tSpeed: 5890.61 samples/sec\tloss=212.612553\n",
      "16:21:17 [INFO]: Epoch[0] Batch [240]\tSpeed: 5905.87 samples/sec\tloss=211.860169\n",
      "16:21:17 [INFO]: Epoch[0] Train-loss=214.048730\n",
      "16:21:17 [INFO]: Epoch[0] Time cost=9.063\n",
      "16:21:17 [INFO]: Saved checkpoint to \"vae-0001.params\"\n",
      "16:21:18 [INFO]: Epoch[1] Batch [20]\tSpeed: 5906.80 samples/sec\tloss=212.018389\n",
      "16:21:18 [INFO]: Epoch[1] Batch [40]\tSpeed: 5905.38 samples/sec\tloss=212.341082\n",
      "16:21:19 [INFO]: Epoch[1] Batch [60]\tSpeed: 5921.10 samples/sec\tloss=212.157645\n",
      "16:21:20 [INFO]: Epoch[1] Batch [80]\tSpeed: 5911.01 samples/sec\tloss=213.315129\n",
      "16:21:20 [INFO]: Epoch[1] Batch [100]\tSpeed: 5914.72 samples/sec\tloss=212.199896\n",
      "16:21:21 [INFO]: Epoch[1] Batch [120]\tSpeed: 4484.12 samples/sec\tloss=211.202970\n",
      "16:21:22 [INFO]: Epoch[1] Batch [140]\tSpeed: 5221.64 samples/sec\tloss=211.261036\n",
      "16:21:23 [INFO]: Epoch[1] Batch [160]\tSpeed: 5364.62 samples/sec\tloss=212.863244\n",
      "16:21:23 [INFO]: Epoch[1] Batch [180]\tSpeed: 5382.04 samples/sec\tloss=213.360202\n",
      "16:21:24 [INFO]: Epoch[1] Batch [200]\tSpeed: 5317.85 samples/sec\tloss=210.247608\n",
      "16:21:25 [INFO]: Epoch[1] Batch [220]\tSpeed: 5650.79 samples/sec\tloss=211.583115\n",
      "16:21:26 [INFO]: Epoch[1] Batch [240]\tSpeed: 5378.76 samples/sec\tloss=211.147083\n",
      "16:21:26 [INFO]: Epoch[1] Train-loss=212.931207\n",
      "16:21:26 [INFO]: Epoch[1] Time cost=9.119\n",
      "16:21:26 [INFO]: Saved checkpoint to \"vae-0002.params\"\n",
      "16:21:27 [INFO]: Epoch[2] Batch [20]\tSpeed: 5547.80 samples/sec\tloss=211.811941\n",
      "16:21:28 [INFO]: Epoch[2] Batch [40]\tSpeed: 5369.57 samples/sec\tloss=210.908023\n",
      "16:21:28 [INFO]: Epoch[2] Batch [60]\tSpeed: 5396.06 samples/sec\tloss=210.934156\n",
      "16:21:29 [INFO]: Epoch[2] Batch [80]\tSpeed: 5421.38 samples/sec\tloss=211.579817\n",
      "16:21:30 [INFO]: Epoch[2] Batch [100]\tSpeed: 5133.21 samples/sec\tloss=211.295194\n",
      "16:21:31 [INFO]: Epoch[2] Batch [120]\tSpeed: 5116.41 samples/sec\tloss=210.358309\n",
      "16:21:31 [INFO]: Epoch[2] Batch [140]\tSpeed: 5005.20 samples/sec\tloss=210.712215\n",
      "16:21:32 [INFO]: Epoch[2] Batch [160]\tSpeed: 5156.03 samples/sec\tloss=211.001146\n",
      "16:21:33 [INFO]: Epoch[2] Batch [180]\tSpeed: 5215.89 samples/sec\tloss=210.908724\n",
      "16:21:34 [INFO]: Epoch[2] Batch [200]\tSpeed: 5384.85 samples/sec\tloss=208.492616\n",
      "16:21:34 [INFO]: Epoch[2] Batch [220]\tSpeed: 5663.73 samples/sec\tloss=209.372893\n",
      "16:21:35 [INFO]: Epoch[2] Batch [240]\tSpeed: 5900.00 samples/sec\tloss=208.715211\n",
      "16:21:35 [INFO]: Epoch[2] Train-loss=210.916801\n",
      "16:21:35 [INFO]: Epoch[2] Time cost=9.314\n",
      "16:21:35 [INFO]: Saved checkpoint to \"vae-0003.params\"\n",
      "16:21:36 [INFO]: Epoch[3] Batch [20]\tSpeed: 5891.82 samples/sec\tloss=208.918687\n",
      "16:21:37 [INFO]: Epoch[3] Batch [40]\tSpeed: 5911.95 samples/sec\tloss=209.018513\n",
      "16:21:37 [INFO]: Epoch[3] Batch [60]\tSpeed: 5916.25 samples/sec\tloss=208.588531\n",
      "16:21:38 [INFO]: Epoch[3] Batch [80]\tSpeed: 5909.85 samples/sec\tloss=208.075725\n",
      "16:21:39 [INFO]: Epoch[3] Batch [100]\tSpeed: 5921.22 samples/sec\tloss=207.857067\n",
      "16:21:39 [INFO]: Epoch[3] Batch [120]\tSpeed: 5916.02 samples/sec\tloss=205.863847\n",
      "16:21:40 [INFO]: Epoch[3] Batch [140]\tSpeed: 5890.65 samples/sec\tloss=205.880376\n",
      "16:21:41 [INFO]: Epoch[3] Batch [160]\tSpeed: 5890.32 samples/sec\tloss=206.159253\n",
      "16:21:42 [INFO]: Epoch[3] Batch [180]\tSpeed: 4528.93 samples/sec\tloss=206.035876\n",
      "16:21:42 [INFO]: Epoch[3] Batch [200]\tSpeed: 5165.85 samples/sec\tloss=201.971011\n",
      "16:21:43 [INFO]: Epoch[3] Batch [220]\tSpeed: 5214.09 samples/sec\tloss=201.975579\n",
      "16:21:44 [INFO]: Epoch[3] Batch [240]\tSpeed: 5147.61 samples/sec\tloss=200.364580\n",
      "16:21:44 [INFO]: Epoch[3] Train-loss=201.517441\n",
      "16:21:44 [INFO]: Epoch[3] Time cost=8.988\n",
      "16:21:44 [INFO]: Saved checkpoint to \"vae-0004.params\"\n",
      "16:21:45 [INFO]: Epoch[4] Batch [20]\tSpeed: 5613.84 samples/sec\tloss=199.328818\n",
      "16:21:46 [INFO]: Epoch[4] Batch [40]\tSpeed: 5478.29 samples/sec\tloss=197.680216\n",
      "16:21:47 [INFO]: Epoch[4] Batch [60]\tSpeed: 5621.79 samples/sec\tloss=196.994225\n",
      "16:21:47 [INFO]: Epoch[4] Batch [80]\tSpeed: 5339.40 samples/sec\tloss=196.438539\n",
      "16:21:48 [INFO]: Epoch[4] Batch [100]\tSpeed: 5349.72 samples/sec\tloss=195.897960\n",
      "16:21:49 [INFO]: Epoch[4] Batch [120]\tSpeed: 5774.61 samples/sec\tloss=194.505680\n",
      "16:21:50 [INFO]: Epoch[4] Batch [140]\tSpeed: 5311.97 samples/sec\tloss=194.745791\n",
      "16:21:50 [INFO]: Epoch[4] Batch [160]\tSpeed: 5010.93 samples/sec\tloss=195.196683\n",
      "16:21:51 [INFO]: Epoch[4] Batch [180]\tSpeed: 5124.83 samples/sec\tloss=194.907688\n",
      "16:21:52 [INFO]: Epoch[4] Batch [200]\tSpeed: 5178.33 samples/sec\tloss=191.363961\n",
      "16:21:53 [INFO]: Epoch[4] Batch [220]\tSpeed: 5128.12 samples/sec\tloss=192.832672\n",
      "16:21:53 [INFO]: Epoch[4] Batch [240]\tSpeed: 5345.36 samples/sec\tloss=192.651328\n",
      "16:21:54 [INFO]: Epoch[4] Train-loss=194.206354\n",
      "16:21:54 [INFO]: Epoch[4] Time cost=9.343\n",
      "16:21:54 [INFO]: Saved checkpoint to \"vae-0005.params\"\n",
      "16:21:55 [INFO]: Epoch[5] Batch [20]\tSpeed: 5565.35 samples/sec\tloss=192.885783\n",
      "16:21:55 [INFO]: Epoch[5] Batch [40]\tSpeed: 5901.95 samples/sec\tloss=192.376079\n",
      "16:21:56 [INFO]: Epoch[5] Batch [60]\tSpeed: 5902.34 samples/sec\tloss=192.662554\n",
      "16:21:57 [INFO]: Epoch[5] Batch [80]\tSpeed: 5838.05 samples/sec\tloss=192.649522\n",
      "16:21:57 [INFO]: Epoch[5] Batch [100]\tSpeed: 5854.61 samples/sec\tloss=192.391771\n",
      "16:21:58 [INFO]: Epoch[5] Batch [120]\tSpeed: 5870.65 samples/sec\tloss=191.476750\n",
      "16:21:59 [INFO]: Epoch[5] Batch [140]\tSpeed: 5891.82 samples/sec\tloss=191.776458\n",
      "16:21:59 [INFO]: Epoch[5] Batch [160]\tSpeed: 5903.89 samples/sec\tloss=192.288489\n",
      "16:22:00 [INFO]: Epoch[5] Batch [180]\tSpeed: 5903.09 samples/sec\tloss=192.128964\n",
      "16:22:01 [INFO]: Epoch[5] Batch [200]\tSpeed: 5885.69 samples/sec\tloss=188.741808\n",
      "16:22:01 [INFO]: Epoch[5] Batch [220]\tSpeed: 5914.00 samples/sec\tloss=190.140229\n",
      "16:22:02 [INFO]: Epoch[5] Batch [240]\tSpeed: 4499.95 samples/sec\tloss=190.148515\n",
      "16:22:03 [INFO]: Epoch[5] Train-loss=192.121230\n",
      "16:22:03 [INFO]: Epoch[5] Time cost=8.796\n",
      "16:22:03 [INFO]: Saved checkpoint to \"vae-0006.params\"\n",
      "16:22:03 [INFO]: Epoch[6] Batch [20]\tSpeed: 5377.43 samples/sec\tloss=190.883822\n",
      "16:22:04 [INFO]: Epoch[6] Batch [40]\tSpeed: 5230.21 samples/sec\tloss=190.616033\n",
      "16:22:05 [INFO]: Epoch[6] Batch [60]\tSpeed: 5375.74 samples/sec\tloss=191.124606\n",
      "16:22:06 [INFO]: Epoch[6] Batch [80]\tSpeed: 5371.68 samples/sec\tloss=190.872968\n",
      "16:22:06 [INFO]: Epoch[6] Batch [100]\tSpeed: 5447.82 samples/sec\tloss=190.447287\n",
      "16:22:07 [INFO]: Epoch[6] Batch [120]\tSpeed: 5492.83 samples/sec\tloss=189.693475\n",
      "16:22:08 [INFO]: Epoch[6] Batch [140]\tSpeed: 5653.80 samples/sec\tloss=189.993100\n",
      "16:22:09 [INFO]: Epoch[6] Batch [160]\tSpeed: 5366.51 samples/sec\tloss=190.203333\n",
      "16:22:09 [INFO]: Epoch[6] Batch [180]\tSpeed: 5391.74 samples/sec\tloss=189.646024\n",
      "16:22:10 [INFO]: Epoch[6] Batch [200]\tSpeed: 5134.37 samples/sec\tloss=186.039804\n",
      "16:22:11 [INFO]: Epoch[6] Batch [220]\tSpeed: 4961.91 samples/sec\tloss=187.225423\n",
      "16:22:12 [INFO]: Epoch[6] Batch [240]\tSpeed: 4693.68 samples/sec\tloss=186.636734\n",
      "16:22:12 [INFO]: Epoch[6] Train-loss=188.353958\n",
      "16:22:12 [INFO]: Epoch[6] Time cost=9.538\n",
      "16:22:12 [INFO]: Saved checkpoint to \"vae-0007.params\"\n",
      "16:22:13 [INFO]: Epoch[7] Batch [20]\tSpeed: 4423.23 samples/sec\tloss=186.818055\n",
      "16:22:14 [INFO]: Epoch[7] Batch [40]\tSpeed: 5129.89 samples/sec\tloss=185.908553\n",
      "16:22:15 [INFO]: Epoch[7] Batch [60]\tSpeed: 5529.42 samples/sec\tloss=186.402412\n",
      "16:22:15 [INFO]: Epoch[7] Batch [80]\tSpeed: 5119.05 samples/sec\tloss=185.457233\n",
      "16:22:16 [INFO]: Epoch[7] Batch [100]\tSpeed: 5461.24 samples/sec\tloss=185.483661\n",
      "16:22:17 [INFO]: Epoch[7] Batch [120]\tSpeed: 5565.00 samples/sec\tloss=184.388399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:22:18 [INFO]: Epoch[7] Batch [140]\tSpeed: 5378.96 samples/sec\tloss=184.682191\n",
      "16:22:18 [INFO]: Epoch[7] Batch [160]\tSpeed: 5744.76 samples/sec\tloss=185.329065\n",
      "16:22:19 [INFO]: Epoch[7] Batch [180]\tSpeed: 5851.14 samples/sec\tloss=184.702756\n",
      "16:22:20 [INFO]: Epoch[7] Batch [200]\tSpeed: 5611.15 samples/sec\tloss=181.590709\n",
      "16:22:20 [INFO]: Epoch[7] Batch [220]\tSpeed: 5504.30 samples/sec\tloss=183.195837\n",
      "16:22:21 [INFO]: Epoch[7] Batch [240]\tSpeed: 5363.83 samples/sec\tloss=182.803973\n",
      "16:22:21 [INFO]: Epoch[7] Train-loss=184.448993\n",
      "16:22:21 [INFO]: Epoch[7] Time cost=9.326\n",
      "16:22:21 [INFO]: Saved checkpoint to \"vae-0008.params\"\n",
      "16:22:22 [INFO]: Epoch[8] Batch [20]\tSpeed: 4193.76 samples/sec\tloss=183.490258\n",
      "16:22:23 [INFO]: Epoch[8] Batch [40]\tSpeed: 4388.28 samples/sec\tloss=182.760764\n",
      "16:22:24 [INFO]: Epoch[8] Batch [60]\tSpeed: 5195.92 samples/sec\tloss=183.493678\n",
      "16:22:25 [INFO]: Epoch[8] Batch [80]\tSpeed: 4859.49 samples/sec\tloss=183.101254\n",
      "16:22:26 [INFO]: Epoch[8] Batch [100]\tSpeed: 5702.01 samples/sec\tloss=182.810200\n",
      "16:22:26 [INFO]: Epoch[8] Batch [120]\tSpeed: 5328.40 samples/sec\tloss=181.845768\n",
      "16:22:27 [INFO]: Epoch[8] Batch [140]\tSpeed: 5022.36 samples/sec\tloss=182.343687\n",
      "16:22:28 [INFO]: Epoch[8] Batch [160]\tSpeed: 4913.09 samples/sec\tloss=183.069964\n",
      "16:22:29 [INFO]: Epoch[8] Batch [180]\tSpeed: 5731.54 samples/sec\tloss=182.637197\n",
      "16:22:29 [INFO]: Epoch[8] Batch [200]\tSpeed: 5287.01 samples/sec\tloss=179.419704\n",
      "16:22:30 [INFO]: Epoch[8] Batch [220]\tSpeed: 4863.15 samples/sec\tloss=180.853091\n",
      "16:22:31 [INFO]: Epoch[8] Batch [240]\tSpeed: 5174.95 samples/sec\tloss=180.904393\n",
      "16:22:31 [INFO]: Epoch[8] Train-loss=182.543305\n",
      "16:22:32 [INFO]: Epoch[8] Time cost=10.014\n",
      "16:22:32 [INFO]: Saved checkpoint to \"vae-0009.params\"\n",
      "16:22:32 [INFO]: Epoch[9] Batch [20]\tSpeed: 4807.43 samples/sec\tloss=181.469567\n",
      "16:22:33 [INFO]: Epoch[9] Batch [40]\tSpeed: 5004.45 samples/sec\tloss=180.541166\n",
      "16:22:34 [INFO]: Epoch[9] Batch [60]\tSpeed: 5124.90 samples/sec\tloss=181.557510\n",
      "16:22:35 [INFO]: Epoch[9] Batch [80]\tSpeed: 5187.27 samples/sec\tloss=181.113191\n",
      "16:22:35 [INFO]: Epoch[9] Batch [100]\tSpeed: 5292.19 samples/sec\tloss=180.763748\n",
      "16:22:36 [INFO]: Epoch[9] Batch [120]\tSpeed: 5558.71 samples/sec\tloss=179.995785\n",
      "16:22:37 [INFO]: Epoch[9] Batch [140]\tSpeed: 5756.02 samples/sec\tloss=180.465731\n",
      "16:22:38 [INFO]: Epoch[9] Batch [160]\tSpeed: 5817.81 samples/sec\tloss=180.971513\n",
      "16:22:38 [INFO]: Epoch[9] Batch [180]\tSpeed: 5817.13 samples/sec\tloss=180.925978\n",
      "16:22:39 [INFO]: Epoch[9] Batch [200]\tSpeed: 5898.96 samples/sec\tloss=177.603854\n",
      "16:22:40 [INFO]: Epoch[9] Batch [220]\tSpeed: 5892.39 samples/sec\tloss=179.102154\n",
      "16:22:40 [INFO]: Epoch[9] Batch [240]\tSpeed: 5731.09 samples/sec\tloss=179.177037\n",
      "16:22:41 [INFO]: Epoch[9] Train-loss=180.588008\n",
      "16:22:41 [INFO]: Epoch[9] Time cost=9.183\n",
      "16:22:41 [INFO]: Saved checkpoint to \"vae-0010.params\"\n",
      "16:22:41 [INFO]: Epoch[10] Batch [20]\tSpeed: 5827.31 samples/sec\tloss=179.662491\n",
      "16:22:42 [INFO]: Epoch[10] Batch [40]\tSpeed: 5440.84 samples/sec\tloss=178.727385\n",
      "16:22:43 [INFO]: Epoch[10] Batch [60]\tSpeed: 4544.48 samples/sec\tloss=179.673212\n",
      "16:22:44 [INFO]: Epoch[10] Batch [80]\tSpeed: 4204.57 samples/sec\tloss=179.155761\n",
      "16:22:45 [INFO]: Epoch[10] Batch [100]\tSpeed: 5084.68 samples/sec\tloss=178.858525\n",
      "16:22:46 [INFO]: Epoch[10] Batch [120]\tSpeed: 5610.87 samples/sec\tloss=178.130673\n",
      "16:22:46 [INFO]: Epoch[10] Batch [140]\tSpeed: 5252.68 samples/sec\tloss=178.366334\n",
      "16:22:47 [INFO]: Epoch[10] Batch [160]\tSpeed: 4998.87 samples/sec\tloss=178.841124\n",
      "16:22:48 [INFO]: Epoch[10] Batch [180]\tSpeed: 5024.87 samples/sec\tloss=178.538985\n",
      "16:22:49 [INFO]: Epoch[10] Batch [200]\tSpeed: 5120.12 samples/sec\tloss=175.490596\n",
      "16:22:49 [INFO]: Epoch[10] Batch [220]\tSpeed: 5647.62 samples/sec\tloss=176.963504\n",
      "16:22:50 [INFO]: Epoch[10] Batch [240]\tSpeed: 5321.74 samples/sec\tloss=176.717137\n",
      "16:22:50 [INFO]: Epoch[10] Train-loss=178.678307\n",
      "16:22:50 [INFO]: Epoch[10] Time cost=9.719\n",
      "16:22:50 [INFO]: Saved checkpoint to \"vae-0011.params\"\n",
      "16:22:51 [INFO]: Epoch[11] Batch [20]\tSpeed: 5429.31 samples/sec\tloss=177.376432\n",
      "16:22:52 [INFO]: Epoch[11] Batch [40]\tSpeed: 5189.50 samples/sec\tloss=176.284571\n",
      "16:22:53 [INFO]: Epoch[11] Batch [60]\tSpeed: 5340.22 samples/sec\tloss=177.112124\n",
      "16:22:54 [INFO]: Epoch[11] Batch [80]\tSpeed: 5146.67 samples/sec\tloss=176.454944\n",
      "16:22:54 [INFO]: Epoch[11] Batch [100]\tSpeed: 5119.15 samples/sec\tloss=175.854781\n",
      "16:22:55 [INFO]: Epoch[11] Batch [120]\tSpeed: 5127.89 samples/sec\tloss=175.141035\n",
      "16:22:56 [INFO]: Epoch[11] Batch [140]\tSpeed: 5349.74 samples/sec\tloss=174.822220\n",
      "16:22:57 [INFO]: Epoch[11] Batch [160]\tSpeed: 5605.02 samples/sec\tloss=174.813929\n",
      "16:22:57 [INFO]: Epoch[11] Batch [180]\tSpeed: 5892.54 samples/sec\tloss=174.223562\n",
      "16:22:58 [INFO]: Epoch[11] Batch [200]\tSpeed: 5805.82 samples/sec\tloss=170.469857\n",
      "16:22:59 [INFO]: Epoch[11] Batch [220]\tSpeed: 5716.59 samples/sec\tloss=171.068009\n",
      "16:22:59 [INFO]: Epoch[11] Batch [240]\tSpeed: 5906.50 samples/sec\tloss=170.464115\n",
      "16:23:00 [INFO]: Epoch[11] Train-loss=172.592415\n",
      "16:23:00 [INFO]: Epoch[11] Time cost=9.152\n",
      "16:23:00 [INFO]: Saved checkpoint to \"vae-0012.params\"\n",
      "16:23:00 [INFO]: Epoch[12] Batch [20]\tSpeed: 5902.74 samples/sec\tloss=170.433811\n",
      "16:23:01 [INFO]: Epoch[12] Batch [40]\tSpeed: 5903.27 samples/sec\tloss=168.733313\n",
      "16:23:02 [INFO]: Epoch[12] Batch [60]\tSpeed: 5900.63 samples/sec\tloss=169.492137\n",
      "16:23:02 [INFO]: Epoch[12] Batch [80]\tSpeed: 5908.86 samples/sec\tloss=168.506920\n",
      "16:23:03 [INFO]: Epoch[12] Batch [100]\tSpeed: 5907.70 samples/sec\tloss=167.899937\n",
      "16:23:04 [INFO]: Epoch[12] Batch [120]\tSpeed: 4019.26 samples/sec\tloss=167.051651\n",
      "16:23:05 [INFO]: Epoch[12] Batch [140]\tSpeed: 5255.73 samples/sec\tloss=167.128140\n",
      "16:23:06 [INFO]: Epoch[12] Batch [160]\tSpeed: 5231.50 samples/sec\tloss=166.704325\n",
      "16:23:06 [INFO]: Epoch[12] Batch [180]\tSpeed: 5515.44 samples/sec\tloss=166.503207\n",
      "16:23:07 [INFO]: Epoch[12] Batch [200]\tSpeed: 5513.73 samples/sec\tloss=163.294750\n",
      "16:23:08 [INFO]: Epoch[12] Batch [220]\tSpeed: 5183.76 samples/sec\tloss=164.248062\n",
      "16:23:09 [INFO]: Epoch[12] Batch [240]\tSpeed: 5116.74 samples/sec\tloss=163.908159\n",
      "16:23:09 [INFO]: Epoch[12] Train-loss=166.165445\n",
      "16:23:09 [INFO]: Epoch[12] Time cost=9.309\n",
      "16:23:09 [INFO]: Saved checkpoint to \"vae-0013.params\"\n",
      "16:23:10 [INFO]: Epoch[13] Batch [20]\tSpeed: 5223.16 samples/sec\tloss=164.227873\n",
      "16:23:11 [INFO]: Epoch[13] Batch [40]\tSpeed: 5019.27 samples/sec\tloss=162.513981\n",
      "16:23:11 [INFO]: Epoch[13] Batch [60]\tSpeed: 5367.21 samples/sec\tloss=163.712674\n",
      "16:23:12 [INFO]: Epoch[13] Batch [80]\tSpeed: 5165.72 samples/sec\tloss=163.024115\n",
      "16:23:13 [INFO]: Epoch[13] Batch [100]\tSpeed: 5124.49 samples/sec\tloss=162.267322\n",
      "16:23:14 [INFO]: Epoch[13] Batch [120]\tSpeed: 4484.39 samples/sec\tloss=161.601062\n",
      "16:23:15 [INFO]: Epoch[13] Batch [140]\tSpeed: 5021.83 samples/sec\tloss=161.605663\n",
      "16:23:15 [INFO]: Epoch[13] Batch [160]\tSpeed: 4934.85 samples/sec\tloss=161.385552\n",
      "16:23:16 [INFO]: Epoch[13] Batch [180]\tSpeed: 4693.48 samples/sec\tloss=161.215362\n",
      "16:23:17 [INFO]: Epoch[13] Batch [200]\tSpeed: 5318.63 samples/sec\tloss=158.507201\n",
      "16:23:18 [INFO]: Epoch[13] Batch [220]\tSpeed: 5437.34 samples/sec\tloss=159.168273\n",
      "16:23:18 [INFO]: Epoch[13] Batch [240]\tSpeed: 5485.83 samples/sec\tloss=158.913935\n",
      "16:23:19 [INFO]: Epoch[13] Train-loss=161.267446\n",
      "16:23:19 [INFO]: Epoch[13] Time cost=9.817\n",
      "16:23:19 [INFO]: Saved checkpoint to \"vae-0014.params\"\n",
      "16:23:20 [INFO]: Epoch[14] Batch [20]\tSpeed: 5447.33 samples/sec\tloss=159.167254\n",
      "16:23:20 [INFO]: Epoch[14] Batch [40]\tSpeed: 5598.24 samples/sec\tloss=157.402629\n",
      "16:23:21 [INFO]: Epoch[14] Batch [60]\tSpeed: 5581.14 samples/sec\tloss=158.816262\n",
      "16:23:22 [INFO]: Epoch[14] Batch [80]\tSpeed: 5584.39 samples/sec\tloss=157.817454\n",
      "16:23:22 [INFO]: Epoch[14] Batch [100]\tSpeed: 5654.53 samples/sec\tloss=156.802078\n",
      "16:23:23 [INFO]: Epoch[14] Batch [120]\tSpeed: 5409.92 samples/sec\tloss=155.902706\n",
      "16:23:24 [INFO]: Epoch[14] Batch [140]\tSpeed: 4719.72 samples/sec\tloss=155.773613\n",
      "16:23:25 [INFO]: Epoch[14] Batch [160]\tSpeed: 3959.84 samples/sec\tloss=155.186454\n",
      "16:23:26 [INFO]: Epoch[14] Batch [180]\tSpeed: 5177.71 samples/sec\tloss=155.054330\n",
      "16:23:27 [INFO]: Epoch[14] Batch [200]\tSpeed: 4999.36 samples/sec\tloss=152.222022\n",
      "16:23:27 [INFO]: Epoch[14] Batch [220]\tSpeed: 5305.20 samples/sec\tloss=152.695786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:23:28 [INFO]: Epoch[14] Batch [240]\tSpeed: 5198.52 samples/sec\tloss=152.290884\n",
      "16:23:28 [INFO]: Epoch[14] Train-loss=154.765404\n",
      "16:23:28 [INFO]: Epoch[14] Time cost=9.665\n",
      "16:23:29 [INFO]: Saved checkpoint to \"vae-0015.params\"\n",
      "16:23:29 [INFO]: Epoch[15] Batch [20]\tSpeed: 5243.42 samples/sec\tloss=152.705669\n",
      "16:23:30 [INFO]: Epoch[15] Batch [40]\tSpeed: 5304.12 samples/sec\tloss=151.027299\n",
      "16:23:31 [INFO]: Epoch[15] Batch [60]\tSpeed: 5253.09 samples/sec\tloss=152.285991\n",
      "16:23:32 [INFO]: Epoch[15] Batch [80]\tSpeed: 5333.53 samples/sec\tloss=151.639048\n",
      "16:23:32 [INFO]: Epoch[15] Batch [100]\tSpeed: 5178.11 samples/sec\tloss=150.608616\n",
      "16:23:33 [INFO]: Epoch[15] Batch [120]\tSpeed: 5269.85 samples/sec\tloss=150.208905\n",
      "16:23:34 [INFO]: Epoch[15] Batch [140]\tSpeed: 5265.14 samples/sec\tloss=150.472818\n",
      "16:23:35 [INFO]: Epoch[15] Batch [160]\tSpeed: 4862.93 samples/sec\tloss=149.971736\n",
      "16:23:36 [INFO]: Epoch[15] Batch [180]\tSpeed: 4612.36 samples/sec\tloss=149.828166\n",
      "16:23:36 [INFO]: Epoch[15] Batch [200]\tSpeed: 4970.85 samples/sec\tloss=147.451598\n",
      "16:23:37 [INFO]: Epoch[15] Batch [220]\tSpeed: 5046.82 samples/sec\tloss=147.728537\n",
      "16:23:38 [INFO]: Epoch[15] Batch [240]\tSpeed: 5565.15 samples/sec\tloss=147.586076\n",
      "16:23:38 [INFO]: Epoch[15] Train-loss=149.843657\n",
      "16:23:38 [INFO]: Epoch[15] Time cost=9.697\n",
      "16:23:38 [INFO]: Saved checkpoint to \"vae-0016.params\"\n",
      "16:23:39 [INFO]: Epoch[16] Batch [20]\tSpeed: 5681.03 samples/sec\tloss=147.967207\n",
      "16:23:40 [INFO]: Epoch[16] Batch [40]\tSpeed: 5726.77 samples/sec\tloss=146.592436\n",
      "16:23:40 [INFO]: Epoch[16] Batch [60]\tSpeed: 5692.91 samples/sec\tloss=147.515380\n",
      "16:23:41 [INFO]: Epoch[16] Batch [80]\tSpeed: 5747.52 samples/sec\tloss=147.059198\n",
      "16:23:42 [INFO]: Epoch[16] Batch [100]\tSpeed: 5682.37 samples/sec\tloss=145.780731\n",
      "16:23:42 [INFO]: Epoch[16] Batch [120]\tSpeed: 5603.61 samples/sec\tloss=145.362950\n",
      "16:23:43 [INFO]: Epoch[16] Batch [140]\tSpeed: 5752.21 samples/sec\tloss=145.796176\n",
      "16:23:44 [INFO]: Epoch[16] Batch [160]\tSpeed: 5731.10 samples/sec\tloss=145.164194\n",
      "16:23:45 [INFO]: Epoch[16] Batch [180]\tSpeed: 4489.28 samples/sec\tloss=145.062471\n",
      "16:23:46 [INFO]: Epoch[16] Batch [200]\tSpeed: 4760.61 samples/sec\tloss=142.532750\n",
      "16:23:46 [INFO]: Epoch[16] Batch [220]\tSpeed: 5081.66 samples/sec\tloss=142.751251\n",
      "16:23:47 [INFO]: Epoch[16] Batch [240]\tSpeed: 4999.57 samples/sec\tloss=142.693435\n",
      "16:23:48 [INFO]: Epoch[16] Train-loss=144.710170\n",
      "16:23:48 [INFO]: Epoch[16] Time cost=9.290\n",
      "16:23:48 [INFO]: Saved checkpoint to \"vae-0017.params\"\n",
      "16:23:48 [INFO]: Epoch[17] Batch [20]\tSpeed: 5447.18 samples/sec\tloss=143.030348\n",
      "16:23:49 [INFO]: Epoch[17] Batch [40]\tSpeed: 5542.08 samples/sec\tloss=141.784709\n",
      "16:23:50 [INFO]: Epoch[17] Batch [60]\tSpeed: 5210.28 samples/sec\tloss=142.483141\n",
      "16:23:51 [INFO]: Epoch[17] Batch [80]\tSpeed: 5539.24 samples/sec\tloss=142.151379\n",
      "16:23:51 [INFO]: Epoch[17] Batch [100]\tSpeed: 5320.07 samples/sec\tloss=141.016647\n",
      "16:23:52 [INFO]: Epoch[17] Batch [120]\tSpeed: 5323.66 samples/sec\tloss=140.559467\n",
      "16:23:53 [INFO]: Epoch[17] Batch [140]\tSpeed: 5277.43 samples/sec\tloss=141.379034\n",
      "16:23:54 [INFO]: Epoch[17] Batch [160]\tSpeed: 5232.90 samples/sec\tloss=140.780709\n",
      "16:23:54 [INFO]: Epoch[17] Batch [180]\tSpeed: 5137.35 samples/sec\tloss=141.168075\n",
      "16:23:55 [INFO]: Epoch[17] Batch [200]\tSpeed: 4759.23 samples/sec\tloss=138.809580\n",
      "16:23:56 [INFO]: Epoch[17] Batch [220]\tSpeed: 5304.41 samples/sec\tloss=139.178280\n",
      "16:23:57 [INFO]: Epoch[17] Batch [240]\tSpeed: 5201.83 samples/sec\tloss=139.267059\n",
      "16:23:57 [INFO]: Epoch[17] Train-loss=141.624768\n",
      "16:23:57 [INFO]: Epoch[17] Time cost=9.532\n",
      "16:23:57 [INFO]: Saved checkpoint to \"vae-0018.params\"\n",
      "16:23:58 [INFO]: Epoch[18] Batch [20]\tSpeed: 5338.14 samples/sec\tloss=139.816630\n",
      "16:23:59 [INFO]: Epoch[18] Batch [40]\tSpeed: 5793.29 samples/sec\tloss=139.059271\n",
      "16:23:59 [INFO]: Epoch[18] Batch [60]\tSpeed: 5871.01 samples/sec\tloss=139.872234\n",
      "16:24:00 [INFO]: Epoch[18] Batch [80]\tSpeed: 5899.48 samples/sec\tloss=139.512921\n",
      "16:24:01 [INFO]: Epoch[18] Batch [100]\tSpeed: 5878.27 samples/sec\tloss=138.532522\n",
      "16:24:01 [INFO]: Epoch[18] Batch [120]\tSpeed: 5896.70 samples/sec\tloss=138.211426\n",
      "16:24:02 [INFO]: Epoch[18] Batch [140]\tSpeed: 5842.01 samples/sec\tloss=138.968190\n",
      "16:24:03 [INFO]: Epoch[18] Batch [160]\tSpeed: 5720.19 samples/sec\tloss=138.339748\n",
      "16:24:03 [INFO]: Epoch[18] Batch [180]\tSpeed: 5829.64 samples/sec\tloss=138.944120\n",
      "16:24:04 [INFO]: Epoch[18] Batch [200]\tSpeed: 5787.16 samples/sec\tloss=136.761976\n",
      "16:24:05 [INFO]: Epoch[18] Batch [220]\tSpeed: 5659.94 samples/sec\tloss=137.047641\n",
      "16:24:06 [INFO]: Epoch[18] Batch [240]\tSpeed: 4055.30 samples/sec\tloss=137.253799\n",
      "16:24:06 [INFO]: Epoch[18] Train-loss=140.013076\n",
      "16:24:06 [INFO]: Epoch[18] Time cost=9.000\n",
      "16:24:06 [INFO]: Saved checkpoint to \"vae-0019.params\"\n",
      "16:24:07 [INFO]: Epoch[19] Batch [20]\tSpeed: 4705.45 samples/sec\tloss=137.839403\n",
      "16:24:08 [INFO]: Epoch[19] Batch [40]\tSpeed: 4853.44 samples/sec\tloss=137.166594\n",
      "16:24:09 [INFO]: Epoch[19] Batch [60]\tSpeed: 5436.42 samples/sec\tloss=137.900184\n",
      "16:24:09 [INFO]: Epoch[19] Batch [80]\tSpeed: 5496.75 samples/sec\tloss=137.636425\n",
      "16:24:10 [INFO]: Epoch[19] Batch [100]\tSpeed: 5274.12 samples/sec\tloss=136.782565\n",
      "16:24:11 [INFO]: Epoch[19] Batch [120]\tSpeed: 5216.20 samples/sec\tloss=136.331916\n",
      "16:24:12 [INFO]: Epoch[19] Batch [140]\tSpeed: 5693.04 samples/sec\tloss=137.197195\n",
      "16:24:12 [INFO]: Epoch[19] Batch [160]\tSpeed: 5350.06 samples/sec\tloss=136.463467\n",
      "16:24:13 [INFO]: Epoch[19] Batch [180]\tSpeed: 5392.03 samples/sec\tloss=137.139813\n",
      "16:24:14 [INFO]: Epoch[19] Batch [200]\tSpeed: 5019.94 samples/sec\tloss=135.058284\n",
      "16:24:15 [INFO]: Epoch[19] Batch [220]\tSpeed: 4841.83 samples/sec\tloss=135.385841\n",
      "16:24:16 [INFO]: Epoch[19] Batch [240]\tSpeed: 4456.72 samples/sec\tloss=135.539805\n",
      "16:24:16 [INFO]: Epoch[19] Train-loss=137.793503\n",
      "16:24:16 [INFO]: Epoch[19] Time cost=9.772\n",
      "16:24:16 [INFO]: Saved checkpoint to \"vae-0020.params\"\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "optimiser = \"adam\"\n",
    "\n",
    "vae_module.fit(train_data=train_iter, optimizer=optimiser, force_init=True, force_rebind=True, num_epoch=epochs,\n",
    "               optimizer_params={'learning_rate': DEFAULT_LEARNING_RATE},\n",
    "               batch_end_callback=mx.callback.Speedometer(frequent=20, batch_size=batch_size),\n",
    "               epoch_end_callback=mx.callback.do_checkpoint('vae'),\n",
    "               eval_metric=\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Data from the VAE\n",
    "\n",
    "Now that we have trained the VAE, we can use it to produce data for us. We first need to load the parameters and use those to construct a new model. We then use that model to produce 10 random digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-c2d63e305cb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_symbol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphantasize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_symbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtest_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vit-tutorial-env/lib/python3.6/site-packages/mxnet/module/module.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, data_shapes, label_shapes, for_training, inputs_need_grad, force_rebind, shared_module, grad_req)\u001b[0m\n\u001b[1;32m    415\u001b[0m                                                      \u001b[0mfixed_param_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fixed_param_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                                                      \u001b[0mgrad_req\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_req\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                                                      state_names=self._state_names)\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_exec_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exec_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_exec_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshared_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vit-tutorial-env/lib/python3.6/site-packages/mxnet/module/executor_group.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, symbol, contexts, workload, data_shapes, label_shapes, param_names, for_training, inputs_need_grad, shared_group, logger, fixed_param_names, grad_req, state_names)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecide_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vit-tutorial-env/lib/python3.6/site-packages/mxnet/module/executor_group.py\u001b[0m in \u001b[0;36mbind_exec\u001b[0;34m(self, data_shapes, label_shapes, shared_group, reshape)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# calculate workload and bind executors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_layouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecide_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel_shapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;31m# call it to make sure labels has the same batch size as data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vit-tutorial-env/lib/python3.6/site-packages/mxnet/module/executor_group.py\u001b[0m in \u001b[0;36mdecide_slices\u001b[0;34m(self, data_shapes)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mspecifying\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \"\"\"\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mmajor_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDataDesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_shapes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "params = mx.nd.load(\"vae-0020.params\")\n",
    "test_vae = construct_vae(latent_type=\"gaussian\", likelihood=\"bernoulliProd\", generator_layer_sizes=[200,500],\n",
    "                   infer_layer_sizes=[500,200], latent_variable_size=200, data_dims=784, generator_act_type=\"tanh\",\n",
    "                   infer_act_type=\"tanh\")\n",
    "\n",
    "num_samples = 10\n",
    "pixels = 784\n",
    "test_data = mx.nd.zeros(shape=(num_samples, 784))\n",
    "\n",
    "test_symbol = vae.phantasize(10)\n",
    "test_module = mx.module.Module(test_symbol, data_names=None, label_names=None)\n",
    "test_module.bind(data_shapes={}, label_shapes={})\n",
    "test_module.set_params(arg_params=params, aux_params=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a better VAE\n",
    "Up to now we have made a mean field assumption, meaning that all dimensions of our latent variable were uncorrelated. This is a very limiting (and unrealistic) assumption, of course. In the following, we are going to fully correlate the dimensions of our latent variable. We achieve this by replacing the vector of standard deviations with a lower-triangular matrix known as a [cholesky factor](https://en.wikipedia.org/wiki/Cholesky_decomposition). This matrix is unconstrained and can thus be computed from a simple feed-forward layer (here we chose a one-hidden-layer net to introduce some non-linearity). Below is some code that will compute that matrix for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cholesky_factor(data: mx.sym.Symbol, latent_dim: int) -> mx.sym.Symbol:\n",
    "    \"\"\"\n",
    "    :param data: The input data. Shape: (batch_size, num_features).\n",
    "    :param size: Dimensionality of the latent variable.\n",
    "    \"\"\"\n",
    "    size = latent_dim ** 2\n",
    "    row_lengths = mx.sym.arange(1, latent_dim + 1)\n",
    "    cholesky_vector = mx.sym.FullyConnected(data=data, num_hidden=latent_dim * 2)\n",
    "    cholesky_vector = mx.sym.Activation(data=cholesky_vector, act_type=\"tanh\")\n",
    "    cholesky_vector = mx.sym.FullyConnected(data=cholesky_vector, num_hidden=size)\n",
    "\n",
    "    # cholesky_vector: (latent_dim, latent_dim, batch_size)\n",
    "    cholesky_factor = mx.sym.reshape(mx.sym.transpose(cholesky_vector),\n",
    "                                     shape=(self.latent_dim, self.latent_dim, -1),\n",
    "                                     name=\"%sreshape_choleksy_vector\" % self.prefix)\n",
    "    # mask rows\n",
    "    cholesky_factor = mx.sym.SequenceMask(data=cholesky_factor, sequence_length=row_lengths,\n",
    "                                          use_sequence_length=True, name=\"%smask_choleksy_rows\" % self.prefix)\n",
    "    # (batch_size, latent_dim, latent_dim)\n",
    "    cholesky_factor = mx.sym.transpose(data=cholesky_factor, name=\"%sbatch_cholesky_factor\" % self.prefix)\n",
    "    return cholesky_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (Extra)\n",
    "Rewrite the GaussianInferenceNet so that it uses the cholesky factor when computing its latent value. You will need the [batch_dot symbol](https://mxnet.incubator.apache.org/api/python/symbol/symbol.html#mxnet.symbol.batch_dot) for this operation. You will also need to replace the KL-term in Gaussian VAE. The KL for the full-covariance multivariate Gaussian is given below (we again assume a standard normal prior). When you are done, build a new VAE using the new code and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_rank_gaussian_kl(mean: mx.sym.Symbol, cholesky_factor: mx.sym.Symbol) -> mx.sym.Symbol:\n",
    "    cov = mx.sym.batch_dot(lhs=cholesky_factor, rhs=choleksy_factor, transpose_b=True)\n",
    "    0.5 * (mx.sym.linalg.potri(cholesky_factor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
