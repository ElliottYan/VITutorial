\documentclass[11pt, a4paper]{article}

\usepackage{amsmath, amssymb, hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue, breaklinks=true}

\usepackage[round]{natbib}

\title{Modules}
\date{last modified: \today}
\author{}

\begin{document}

\maketitle

The modules below are in no particular order (except for the Basics, of course).
 
\section{Basics}

\begin{itemize}
\item What is a posterior? $ \rightarrow $ recap of Bayes' rule
\item Example problems: Factorial HMMs, Bayesian Mixture Models (show GMs)
\item ELBO derivation I: from KL divergence
\item ELBO derivation II: with Jensen's inequality
\item Mean Field inference
\item Application to example problems (show GMs)
\end{itemize}

\section{Conjugate Models}
\begin{itemize}
\item Exponential families
\item Gaussian-Gaussian conjugacy
\item Example: Bayesian Linear Regression
\item Dirichlet-multinomial conjugacy
\item Example: Multinomial Mixture Model (maybe even LDA?)
\item Conjugate VI in the general case \citep{Beal:2003}
\end{itemize}

\section{Nonconjugate Models}
\begin{itemize}
\item Laplace Approximation
\item Gradient methods
\item Problem: cannot simply differentiate an MC average
\item Idea: transform $ \frac{d}{dq} \mathbb{E}_{q}[\cdot] $ into $ \mathbb{E}[\frac{d}{dq}\cdot] $
\item Score function gradient $ \rightarrow $ Black Box VI \citep{PaisleyEtAl:2012, RanganathEtAl:2014}
\item Reparametrisation gradient \citep{KingmaWelling:2013, RezendeEtAl:2014, TitsiasLazarogredilla:2014}
\end{itemize}

\section{Nonparametric Models}
\begin{itemize}
\item Intro to stick-breaking processes \citep{IshwaranJames:2001}
\item VI for HDP/PYP \citep{WangEtAl:2011}
\item Intro to GPs
\item VI for GPs
\end{itemize}

\section{Bayesian Neural Networks}
\begin{itemize}
\item Putting priors on weights
\item The old stuff by Neal, MacKay and Hinton \citep{HintonVancamp:1993}
\item The new stuff by DeepMind et al. \citep{Graves:2011, BlundellEtAl:2015}
\item Bayesian Interpretation of Dropout \citep{Gal:2016}
\end{itemize}

\section{Deep Generative Models}
\begin{itemize}
\item Review of generative models
\item Exact case: EM with features
\item Variational Autoencoders \citep{KingmaWelling:2013, RezendeEtAl:2014}
\item Example models: ???
\item Code snippet ???
\item Extra: The Deep Generative CRF (the Ryan Adams paper from NIPS)
\end{itemize}

\section{Reparametrisation Gradients}
\begin{itemize}
\item Recap: Gaussian reparametrisation 
\item Exension to general location-scale families \citep{TitsiasLazarogredilla:2014}
\item ADVI (depending on the audience only go until here; the next two are way more complicated) \citep{KucukelbirEtAl:2017}
\item Generalised Reparametrisation Gradient \citep{RuizEtAl:2016}
\item Rejection Sampling VI \citep{NaessethEtAl:2017}
\end{itemize}

\section{Beyond Mean Field [Advanced]}
\begin{itemize}
\item Structured VI (example: Bayesian or Factorial HMMs)
\item Auxiliary variables
\item Hierarchical Varational models 
\end{itemize}

\section{Collapsed VB}
\begin{itemize}
\item Taylor expansions
\item Example: LDA
\item Connection between collapsed VB and unconstrained variational approximation \citep{TehEtAl:2007}
\item CVB0 \citep{AsuncionEtAl:2009}
\end{itemize}

\section{Beyond KL [Advanced]}
\begin{itemize}
\item $ \alpha $-divergence (make connection to EP)
\item Stein VI
\item Implicit models
\item Hoelder bound
\end{itemize}

\bibliographystyle{plainnat}
\bibliography{VI}

\end{document}