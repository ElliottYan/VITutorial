\documentclass[11pt, a4paper]{article}

\usepackage{amsmath, amssymb}
\usepackage{xcolor}
\usepackage{marginnote}


\newcommand{\pnote}[1]{\textcolor{blue}{#1}}
\newcommand{\wnote}[1]{\textcolor{red}{#1}}

\title{Modules}
\date{last modified: \today}
\author{}

\begin{document}

\maketitle


The modules below are in no particular order (except for the Basics, of course).\footnote{\wnote{The order is actually not too arbitrary :) I noted a few exceptions below. Let's try and come up with an ideal order or some other form of dependency.}}
 
\section{Basics}

\begin{itemize}
\item What is a posterior \wnote{and inference}? $ \rightarrow $ recap of Bayes' rule
\item Example problems: Factorial HMMs, Bayesian Mixture Models (show GMs)
\item \wnote{Is it worth to use sampling as an intuitive way of performing inference before diving in the realms of VI?}
\item ELBO derivation I: from KL divergence
\item ELBO derivation II: with Jensen's inequality
\item \wnote{Connection to EM}
\item Mean Field inference
\item Application to example problems (show GMs)
\end{itemize}

\section{Conjugate Models}

\begin{itemize}
\item Exponential families
\item Gaussian-Gaussian conjugacy
\item Example: Gaussian Mixture Model
\item \wnote{Beta-Binomial warmup?}
\item Dirichlet-multinomial conjugacy
\item Example: Multinomial Mixture Model (maybe even LDA? \wnote{Yes!})
\item Conjugate VI in the general case
\end{itemize}

\section{Nonconjugate Models}
\begin{itemize}
\item Laplace Approximation (\wnote{depending on audience})
\item Gradient methods
\item Problem: cannot simply differentiate an MC average
\item Idea: transform $ \frac{d}{dq} \mathbb{E}_{q}[\cdot] $ into $ \mathbb{E}[\frac{d}{dq}\cdot] $
\item Score function gradient $ \rightarrow $ Black Box VI
\item Reparametrisation gradient
\end{itemize}

\section{Nonparametric Models}

\wnote{I would push this module further down and depending on audience drop it altogether. I suppose people who do not know VI, do not know nonparametric models either. Thus we would need to invest quite some time in this.}

\begin{itemize}
\item Intro to stick-breaking processes
\item VI for HDP/PYP
\item Intro to GPs
\item VI for GPs
\end{itemize}

\section{Bayesian Neural Networks}
\begin{itemize}
\item Putting priors on weights
\item The old stuff by Neal, MacKay and Hinton
\item The new stuff by DeepMind
\item Bayesian Interpretation of Dropout
\end{itemize}

\section{Deep Generative Models}
\begin{itemize}
\item Review of generative models
\item Exact case: EM with features
\item Variational Autoencoders
\item Example models: ???
\item Code snippet ??? \wnote{We can aim to have notebooks for some (or all?) of the modules.}
\item Extra: The Deep Generative CRF (the Ryan Adams paper from NIPS)
\end{itemize}

\section{Reparametrisation Gradients}

\wnote{I think the whole module should depend on audience and we can cover the location-scale case in the modules about Nonconjugate models and/or DGMs.}

\begin{itemize}
\item Recap: Gaussian reparametrisation
\item Exension to general location-scale families
\item ADVI (depending on the audience only go until here; the next two are way more complicated)
\item Generalised Reparametrisation Gradient
\item Rejection Sampling VI
\end{itemize}

\section{Beyond Mean Field [Advanced]}
\begin{itemize}
\item Structured VI (example: Bayesian or Factorial HMMs)
\item Auxiliary variables
\item Hierarchical Varational models 
\end{itemize}

\section{Collapsed VB}

\wnote{Another module that depends on audience: people with Bayesian aspirations vs people who want to play with DGMs.}

\begin{itemize}
\item Taylor expansions
\item Example: LDA
\item Connection between collapsed VB and unconstrained variational approximation
\item CVB0
\end{itemize}

\section{Beyond KL [Advanced\wnote{+}]}
\begin{itemize}
\item $ \alpha $-divergence (make connection to EP)
\item Stein VI
\item Implicit models
\item Hoelder bound
\end{itemize}


\section{Not sure where to fit}

\begin{itemize}
	\item \wnote{Stochastic optimisation: at least at a high level}
	\item \wnote{GAN: if Eric Xing's connection between VAEs and GANs turn out interesting}	
	\item \wnote{I note that NLP2 students (and colleagues of mine) struggle to understand what it means to impose a prior. We can try to clear that out (perhaps in module Conjugate Models).}
	\item \wnote{People are usually ready to quote ``regularisation is an approximate Bayesian prior'' but they do not understand the limits/implications of the word ``approximate'' there and in a way they perceive it as not too different from ``VI is approximate posterior inference''. Perhaps this is worth discussing when we talk about Bayesian interpretations of (stochastic) regularisation techniques in the module BNNs.}
\end{itemize}

\end{document}